<!DOCTYPE html>
<html lang="en-US">
<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="description" content="Image Matching: Local Features &amp; Beyond - CVPR 2019 Workshop">
<meta name="keywords" content="image,matching,local Features, CVPR 2019, Workshop, CVPR">

<base href="https://image-matching-workshop.github.io/">

<title>Image Matching: Local Features &amp; Beyond</title>

<meta name="generator" content="Hugo 0.53" />



<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,400|Roboto+Slab:400,700|Roboto:300,300i,400,400i,500,500i,700,700i">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/all.css">
<link rel="stylesheet" href="https://image-matching-workshop.github.io/css/datatables.min.css">
<link rel="stylesheet" href="https://image-matching-workshop.github.io/css/balloon.min.css">
<link rel="stylesheet" href="https://image-matching-workshop.github.io/css/main.css">
<link rel="stylesheet" href="https://image-matching-workshop.github.io/css/custom.css">




<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="theme-color" content="#ffffff">





<script type="text/javascript" src="https://image-matching-workshop.github.io/js/jquery.latest.min.js"></script>
<script type="text/javascript" src="https://image-matching-workshop.github.io/js/datatables.min.js"></script>
<script>
$(document).ready(function (){
    var table = $('.leaderboard_stereo').DataTable({
        "columnDefs": [{targets:[0, 2], orderSequence: ['desc', 'asc']}],
        "columnDefs": [{targets:[1, 3, 4, 5, 6, 7, 8, 9], orderSequence: ['desc', 'asc']}],
        "order": [[7, 'desc']],
        responsive: {
            details: {
                renderer: function ( api, rowIdx, columns ) {
                    var data = $.map( columns, function ( col, i ) {
                        return col.hidden ?
                            '<tr data-dt-row="'+col.rowIndex+'" data-dt-column="'+col.columnIndex+'">'+
                                '<td class="align_left details_title"><b>'+col.title+':'+'</b></td> '+
                                '<td class="align_left details_data">'+col.data+'</td>'+
                            '</tr>' :
                            '';
                    } ).join('');
 
                    return data ?
                        $('<table/>').append( data ) :
                        false;
                }
            }
        }
    });

    var table = $('.leaderboard_mvs').DataTable({
        "columnDefs": [{targets:[0, 2], orderSequence: ['desc', 'asc']}],
        "columnDefs": [{targets:[1, 3, 4, 5, 6, 7, 8, 9, 10, 11], orderSequence: ['desc', 'asc']}],
        "order": [[9, 'desc']],
        responsive: {
            details: {
                renderer: function ( api, rowIdx, columns ) {
                    var data = $.map( columns, function ( col, i ) {
                        return col.hidden ?
                            '<tr data-dt-row="'+col.rowIndex+'" data-dt-column="'+col.columnIndex+'">'+
                                '<td class="align_left details_title"><b>'+col.title+':'+'</b></td> '+
                                '<td class="align_left details_data">'+col.data+'</td>'+
                            '</tr>' :
                            '';
                    } ).join('');
 
                    return data ?
                        $('<table/>').append( data ) :
                        false;
                }
            }
        }
    });

    var table = $('.checkerboard').DataTable({
        "columnDefs": [{targets:[0], orderSequence: ['desc', 'asc']}],
        "columnDefs": [{targets:[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], orderSequence: ['desc', 'asc']}],
        "order": [[12, 'desc']],
        responsive: {
            details: {
                renderer: function ( api, rowIdx, columns ) {
                    var data = $.map( columns, function ( col, i ) {
                        return col.hidden ?
                            '<tr data-dt-row="'+col.rowIndex+'" data-dt-column="'+col.columnIndex+'">'+
                                '<td class="align_left details_title"><b>'+col.title+':'+'</b></td> '+
                                '<td class="align_left details_data">'+col.data+'</td>'+
                            '</tr>' :
                            '';
                    } ).join('');
 
                    return data ?
                        $('<table/>').append( data ) :
                        false;
                }
            }
        }
    });

    $('.tablelist').dataTable({searching: false, paging: false, info: false});

    

    
    $('#btn-show-all-children').on('click', function(){
        
        table.rows(':not(.parent)').nodes().to$().find('td:first-child').trigger('click');
    });

    
    $('#btn-hide-all-children').on('click', function(){
        
        table.rows('.parent').nodes().to$().find('td:first-child').trigger('click');
    });
});
</script>
</head>
<body lang="en-US">
<div class="container">


<header class="row text-left title">
  <h1 class="title">Leaderboard</h1>
</header>
<section id="category-pane" class="row meta">
  
</section>
<section id="content-pane" class="row">
  <div class="col-md-12 text-justify content">
    

<!--TODO-->

<!--* Add automated images with maching results/3D point clouds, for comparison, like in the Middlebury leaderboard.-->

<p>The challenge contains the following datasets and tasks (<a href="challenge">see this for details</a>):</p>

<ul>
<li><a href="leaderboard#photot_stereo">Task P1: Stereo maching in the &lsquo;Phototourism&rsquo; dataset</a></li>
<li><a href="leaderboard#photot_mvs">Task P2: Multi-view reconstruction in the &lsquo;Phototourism&rsquo; dataset</a></li>
<li><a href="leaderboard#silda">Task S1: Stereo maching in the &lsquo;SILDa Image Matching&rsquo; dataset</a>
<!--* [Glossary](leaderboard#glossary)--></li>
</ul>

<p>Some notes:</p>

<ul>
<li>Place the mouse cursor over row headers for details about the metrics (or <span class="balloon" data-balloon="This is an example."
data-balloon-pos="up">here for an example</span>).</li>
<li>You can filter using the search box and labels, which are listed under the
name of the method. Sparse methods are broken down into categories by the
number of keypoints used: up to 1024, 2048, and 8000 keypoints per image.
Sparse feature matching can be done by brute-force nearest neighbour search
(&ldquo;nn&rdquo;), one to one correspondences (&ldquo;1to1&rdquo;), or user-provided matches.</li>
</ul>

<hr>

<h2 id="a-name-photot-stereo-a-p1-phototourism-dataset-mdash-stereo-task"><a name="photot_stereo"></a>[P1] Phototourism dataset &mdash; Stereo task</h2>

<p>Performance in stereo matching, averaged over all the test sequences.</p>

<ul>
<li><a href="breakdown/phototourism/stereo/by_seq">Click here for a breakdown by sequence</a></li>
</ul>

<p><br />




<table class="display leaderboard_stereo" width="99%" cellspacing="0">
<thead>
<tr>
  <th colspan="10" style="text-align: center;" class="sorter-false">Stereo &mdash; averaged over all sequences</th>
</tr>
<tr>
  
  <th class="all align_left" width="99%">Method</th>
  <th class="all">Date</th>
  <th class="all"><span class="balloon" data-balloon="F: Features only.&#10;M: Features and custom matches.&#10;P: Poses (TBA)." data-balloon-break data-balloon-pos="up">Type</span></th>
  
  <th class="all"><span class="balloon" data-balloon="Average number of keypoints extracted." data-balloon-pos="up">#kp</span></th>
  <th class="all"><span class="balloon" data-balloon="Matching Score." data-balloon-pos="up">MS</span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 5-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>5<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 10-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>10<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 15-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>15<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 20-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>20<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 25-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>25<sup>o</sup></sup></span></th>
  
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
</tr>
</thead>
<tbody>


















<tr>
  
  
  <td class="align_left"><span class="boards_title">AKAZE (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>7879.7</td>
  <td>0.221</td>
  <td>0.001</td>
  <td>0.007</td>
  <td>0.029</td>
  <td>0.073</td>
  <td>0.138</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  
  <td class="align_left"><span class="boards_title">ORB (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>7128.0</td>
  <td>0.224</td>
  <td>0.000</td>
  <td>0.004</td>
  <td>0.019</td>
  <td>0.050</td>
  <td>0.095</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  
  <td class="align_left"><span class="boards_title">SIFT &#43; GeoDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>7885.0</td>
  <td>0.233</td>
  <td>0.001</td>
  <td>0.010</td>
  <td>0.037</td>
  <td>0.091</td>
  <td>0.165</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  
  <td class="align_left"><span class="boards_title">SIFT &#43; HardNet</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>7885.0</td>
  <td>0.252</td>
  <td>0.001</td>
  <td>0.011</td>
  <td>0.043</td>
  <td>0.101</td>
  <td>0.179</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  
  <td class="align_left"><span class="boards_title">SIFT &#43; L2-Net</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>7885.0</td>
  <td>0.248</td>
  <td>0.001</td>
  <td>0.011</td>
  <td>0.040</td>
  <td>0.093</td>
  <td>0.170</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  
  <td class="align_left"><span class="boards_title">SIFT (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>7884.4</td>
  <td>0.215</td>
  <td>0.000</td>
  <td>0.007</td>
  <td>0.028</td>
  <td>0.069</td>
  <td>0.130</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  
  <td class="align_left"><span class="boards_title">SIFT &#43; TFeat</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>7885.0</td>
  <td>0.236</td>
  <td>0.001</td>
  <td>0.010</td>
  <td>0.036</td>
  <td>0.084</td>
  <td>0.154</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">N/A<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>1266.5</td>
  <td>0.258</td>
  <td>0.001</td>
  <td>0.012</td>
  <td>0.041</td>
  <td>0.097</td>
  <td>0.168</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels, and use as many keypoints as the model returns. Feature matching with brute-force nearest-neighbour search. TODO these results will be replaced with a more consistent number of keypoints.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  
  <td class="align_left"><span class="boards_title">SURF (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>7748.8</td>
  <td>0.208</td>
  <td>0.001</td>
  <td>0.006</td>
  <td>0.021</td>
  <td>0.054</td>
  <td>0.106</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>


</tbody>
</table>



<hr></p>

<p><br /></p>

<h2 id="a-name-photot-mvs-a-p2-phototourism-dataset-mdash-multi-view-task"><a name="photot_mvs"></a>[P2] Phototourism dataset &mdash; Multi-view task</h2>

<p>Performance in SfM reconstruction, averaged over all the test sequences.</p>

<ul>
<li><a href="breakdown/phototourism/mvs/by_seq">Click here for a breakdown by sequence</a></li>
<li><a href="breakdown/phototourism/mvs/by_bag">Click here for a breakdown by subset size</a></li>
</ul>

<p><br />










<table class="display leaderboard_mvs" width="99%" cellspacing="0">
<thead>
<tr>
  <th colspan="13" style="text-align: center;" class="sorter-false">MVS &mdash; averaged over all sequences</th>
</tr>
<tr>
  
  <th class="all align_left" width="99%">Method</th>
  <th class="all">Date</th>
  <th class="all"><span class="balloon" data-balloon="F: Features only.&#10;M: Features and custom matches." data-balloon-break data-balloon-pos="up">Type</span></th>
  
  <th class="all"><span class="balloon" data-balloon="Ratio of images successfully&#10;registered (min: 3)." data-balloon-break data-balloon-pos="up">Ims (%)</span></th>
  <th class="all"><span class="balloon" data-balloon="Average number of 3D points&#10;in the reconstruction." data-balloon-break data-balloon-pos="up">#Pts</span></th>
  <th class="all"><span class="balloon" data-balloon="Success ratio in the&#10;3D reconstruction (%)." data-balloon-break data-balloon-pos="up">SR</span></th>
  <th class="all"><span class="balloon" data-balloon="Average Track Length&#10;(# of observations per 3D point)." data-balloon-break data-balloon-pos="up">TL</span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 5-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>5<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 10-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>10<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 15-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>15<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 20-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>20<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 25-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>25<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Average Trajectory Error." data-balloon-break data-balloon-pos="up">ATE</span></th>
  
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
</tr>
</thead>
<tbody>































<tr>
  
  <td class="align_left"><span class="boards_title">AKAZE (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>79.3</td>
  <td>2163.6</td>
  <td>88.5</td>
  <td>2.86</td>
  <td>0.220</td>
  <td>0.306</td>
  <td>0.365</td>
  <td>0.413</td>
  <td>0.451</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ORB (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>72.8</td>
  <td>1432.0</td>
  <td>77.6</td>
  <td>2.59</td>
  <td>0.073</td>
  <td>0.121</td>
  <td>0.160</td>
  <td>0.196</td>
  <td>0.230</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; GeoDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>82.5</td>
  <td>3094.0</td>
  <td>95.8</td>
  <td>2.99</td>
  <td>0.325</td>
  <td>0.421</td>
  <td>0.480</td>
  <td>0.532</td>
  <td>0.571</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; HardNet</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>83.2</td>
  <td>3788.6</td>
  <td>97.2</td>
  <td>2.97</td>
  <td>0.337</td>
  <td>0.439</td>
  <td>0.502</td>
  <td>0.552</td>
  <td>0.593</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; L2-Net</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>82.5</td>
  <td>3313.0</td>
  <td>96.3</td>
  <td>2.90</td>
  <td>0.289</td>
  <td>0.386</td>
  <td>0.452</td>
  <td>0.503</td>
  <td>0.547</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>78.6</td>
  <td>2026.9</td>
  <td>88.5</td>
  <td>2.80</td>
  <td>0.214</td>
  <td>0.292</td>
  <td>0.347</td>
  <td>0.391</td>
  <td>0.429</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; TFeat</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>81.7</td>
  <td>2825.6</td>
  <td>94.1</td>
  <td>2.84</td>
  <td>0.252</td>
  <td>0.346</td>
  <td>0.409</td>
  <td>0.462</td>
  <td>0.506</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">N/A<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>76.9</td>
  <td>570.6</td>
  <td>85.9</td>
  <td>3.03</td>
  <td>0.228</td>
  <td>0.319</td>
  <td>0.374</td>
  <td>0.417</td>
  <td>0.451</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels, and use as many keypoints as the model returns. Feature matching with brute-force nearest-neighbour search. TODO these results will be replaced with a more consistent number of keypoints.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SURF (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">2019-04-24</span></td>
  <td>F</td>
  
  <td>74.9</td>
  <td>1158.8</td>
  <td>80.8</td>
  <td>2.60</td>
  <td>0.116</td>
  <td>0.177</td>
  <td>0.222</td>
  <td>0.260</td>
  <td>0.295</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  

</tbody>
</table>



<hr></p>

<p><br /><br /></p>

<h2 id="a-name-silda-a-s1-silda-dataset-mdash-image-matching-task"><a name="silda"></a>[S1] SILDa dataset &mdash; Image matching task</h2>

<p>Coming Soon.</p>

<p><br /><br />
<!--### <a name="glossary"></a>Glossary--></p>

<!--Stereo task:

<ul>
  <li><b>Type:</b> Input format. "F": Features. "M": Features and matches. "P": Poses.</li>
  <li><b>#Kp:</b> Average number of keypoints extracted.</li>
  <li><b>MS:</b> Matching Score.</li>
  <li><b>mAP<sup>x</sup>:</b> Mean average precision at angular threshold *x*.</li>
</ul>
-->

<!--Multi-view reconstruction task:

<ul>
  <li><b>Type:</b> Input format. "F": Features. "M": Features and matches. "P": Poses.</li>
  <li><b>#Ims:</b> Ratio of images registered by the reconstruction (on successful reconstructions).</li>
  <li><b>#Pts:</b> Average number of 3D points obtained by the reconstruction.</li>
  <li><b>SR:</b> Success ratio (percentage) in the re-construction with COLMAP, out of 100 subsets.</li>
  <li><b>TL:</b> Average track length (number of 3D point observations).</li>
  <li><b>mAP<sup>x</sup>:</b> Mean average precision at angular threshold *x*.</li>
  <li><b>ATE:</b> Absolute Trajectory Error.</li>
</ul>
-->

  </div>
</section>
<section id="tag-pane" class="row meta">
  
</section>








<section id="menu-pane" class="row menu text-center">
  
  
  
  
  
  <h4 class="text-center"><a class="menu-item" href="https://image-matching-workshop.github.io/">home</a></h4>
</section>



<footer class="row text-center footer">
  
  
</footer>

</div>




<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-62863910-6', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="js/main.js"></script>

</script>
</body>
</html>


