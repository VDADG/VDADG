<!DOCTYPE html>
<html lang="en-US">
<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="description" content="Image Matching: Local Features &amp; Beyond - CVPR 2019 Workshop">
<meta name="keywords" content="image,matching,local Features, CVPR 2019, Workshop, CVPR">

<base href="https://image-matching-workshop.github.io/">

<title>Image Matching: Local Features &amp; Beyond</title>

<meta name="generator" content="Hugo 0.53" />



<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,400|Roboto+Slab:400,700|Roboto:300,300i,400,400i,500,500i,700,700i">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/all.css">
<link rel="stylesheet" href="https://image-matching-workshop.github.io/css/datatables.min.css">
<link rel="stylesheet" href="https://image-matching-workshop.github.io/css/main.css">
<link rel="stylesheet" href="https://image-matching-workshop.github.io/css/custom.css">




<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="theme-color" content="#ffffff">





<script type="text/javascript" src="https://image-matching-workshop.github.io/js/jquery.latest.min.js"></script>
<script type="text/javascript" src="https://image-matching-workshop.github.io/js/datatables.min.js"></script>
<script>
$(document).ready(function (){
    var table = $('.leaderboard_stereo').DataTable({
        "columnDefs": [{targets:[0, 2], orderSequence: ['desc', 'asc']}],
        "columnDefs": [{targets:[1, 3, 4, 5, 6, 7, 8, 9], orderSequence: ['desc', 'asc']}],
        "order": [[7, 'desc']],
        responsive: {
            details: {
                renderer: function ( api, rowIdx, columns ) {
                    var data = $.map( columns, function ( col, i ) {
                        return col.hidden ?
                            '<tr data-dt-row="'+col.rowIndex+'" data-dt-column="'+col.columnIndex+'">'+
                                '<td><b>'+col.title+':'+'</b></td> '+
                                '<td>'+col.data+'</td>'+
                            '</tr>' :
                            '';
                    } ).join('');
 
                    return data ?
                        $('<table/>').append( data ) :
                        false;
                }
            }
        }
    });

    var table = $('.leaderboard_mvs').DataTable({
        "columnDefs": [{targets:[0, 2], orderSequence: ['desc', 'asc']}],
        "columnDefs": [{targets:[1, 3, 4, 5, 6, 7, 8, 9, 10, 11], orderSequence: ['desc', 'asc']}],
        "order": [[9, 'desc']],
        responsive: {
            details: {
                renderer: function ( api, rowIdx, columns ) {
                    var data = $.map( columns, function ( col, i ) {
                        return col.hidden ?
                            '<tr data-dt-row="'+col.rowIndex+'" data-dt-column="'+col.columnIndex+'">'+
                                '<td><b>'+col.title+':'+'</b></td> '+
                                '<td>'+col.data+'</td>'+
                            '</tr>' :
                            '';
                    } ).join('');
 
                    return data ?
                        $('<table/>').append( data ) :
                        false;
                }
            }
        }
    });

    var table = $('.checkerboard').DataTable({
        "columnDefs": [{targets:[0], orderSequence: ['desc', 'asc']}],
        "columnDefs": [{targets:[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], orderSequence: ['desc', 'asc']}],
        "order": [[12, 'desc']],
        responsive: {
            details: {
                renderer: function ( api, rowIdx, columns ) {
                    var data = $.map( columns, function ( col, i ) {
                        return col.hidden ?
                            '<tr data-dt-row="'+col.rowIndex+'" data-dt-column="'+col.columnIndex+'">'+
                                '<td><b>'+col.title+':'+'</b></td> '+
                                '<td>'+col.data+'</td>'+
                            '</tr>' :
                            '';
                    } ).join('');
 
                    return data ?
                        $('<table/>').append( data ) :
                        false;
                }
            }
        }
    });

    $('.tablelist').dataTable({searching: false, paging: false, info: false});

    

    
    $('#btn-show-all-children').on('click', function(){
        
        table.rows(':not(.parent)').nodes().to$().find('td:first-child').trigger('click');
    });

    
    $('#btn-hide-all-children').on('click', function(){
        
        table.rows('.parent').nodes().to$().find('td:first-child').trigger('click');
    });
});
</script>
</head>
<body lang="en-US">
<div class="container">


<header class="row text-left title">
  <h1 class="title">Leaderboard</h1>
</header>
<section id="category-pane" class="row meta">
  
</section>
<section id="content-pane" class="row">
  <div class="col-md-12 text-justify content">
    

<!--TODO-->

<!--* Add automated images with maching results/3D point clouds, for comparison, like in the Middlebury leaderboard.-->

<p><span style="color: #c00">This is a work in progress. More baselines/sequences
will be added (by the organizers) over the next 1-2 weeks. Minor alterations to
the test pipeline will happen before the final release of the test data and
evaluation procedure.</span></p>

<p>The 2019 edition of the challenge contains two tasks over two datasets:</p>

<ul>
<li><a href="leaderboard#photot_stereo">Task P1: Stereo maching in the &lsquo;Phototourism&rsquo; dataset</a></li>
<li><a href="leaderboard#photot_mvs">Task P2: Multi-view reconstruction in the &lsquo;Phototourism&rsquo; dataset</a></li>
<li><a href="leaderboard#silda">Task L1: Stereo maching in the &lsquo;SILDa Image Matching&rsquo; dataset</a></li>
<li><a href="leaderboard#glossary">Glossary</a></li>
</ul>

<p>For details about the datasets, tasks, and evaluation metrics, please refer to
the <a href="challenge">datasets page</a>.</p>

<h2 id="a-name-photot-a-dataset-1-phototourism"><a name="photot"></a>Dataset 1: Phototourism</h2>

<h3 id="a-name-photot-stereo-a-task-p1-stereo"><a name="photot_stereo"></a>Task P1: Stereo</h3>

<p>Performance in stereo matching, averaged over all the test sequences.</p>

<hr>

<p>



<table class="display leaderboard_stereo" cellspacing="0" width="100%">
<thead>
<tr>
  <th colspan="10" style="text-align: center;" class="sorter-false">Stereo &mdash; averaged over all sequences</th>
</tr>
<tr>
  
  <th class="all">Method</th>
  <th class="all">Date</th>
  <th class="all">Type</th>
  
  <th class="all">#Kp</th>
  <th class="all">MS</th>
  <th class="all">mAP<sup>5<sup>o</sup></sup></th>
  <th class="all">mAP<sup>10<sup>o</sup></sup></th>
  <th class="all">mAP<sup>15<sup>o</sup></sup></th>
  <th class="all">mAP<sup>20<sup>o</sup></sup></th>
  <th class="all">mAP<sup>25<sup>o</sup></sup></th>
  
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
</tr>
</thead>
<tbody>


















<tr>
  
  <td>AKAZE (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>7879.7</td>
  <td>0.221</td>
  <td>0.001</td>
  <td>0.007</td>
  <td>0.029</td>
  <td>0.073</td>
  <td>0.138</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  <td>ORB (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>7128.0</td>
  <td>0.224</td>
  <td>0.000</td>
  <td>0.004</td>
  <td>0.019</td>
  <td>0.050</td>
  <td>0.095</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  <td>GeoDesc (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>7885.0</td>
  <td>0.233</td>
  <td>0.001</td>
  <td>0.010</td>
  <td>0.037</td>
  <td>0.091</td>
  <td>0.165</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  <td>HardNet (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>7885.0</td>
  <td>0.252</td>
  <td>0.001</td>
  <td>0.011</td>
  <td>0.043</td>
  <td>0.101</td>
  <td>0.179</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  <td>L2-Net (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>7885.0</td>
  <td>0.248</td>
  <td>0.001</td>
  <td>0.011</td>
  <td>0.040</td>
  <td>0.093</td>
  <td>0.170</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  <td>SIFT (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>7884.4</td>
  <td>0.215</td>
  <td>0.000</td>
  <td>0.007</td>
  <td>0.028</td>
  <td>0.069</td>
  <td>0.130</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  <td>TFeat (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>7885.0</td>
  <td>0.236</td>
  <td>0.001</td>
  <td>0.010</td>
  <td>0.036</td>
  <td>0.084</td>
  <td>0.154</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  <td>SuperPoint</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>1266.5</td>
  <td>0.258</td>
  <td>0.001</td>
  <td>0.012</td>
  <td>0.041</td>
  <td>0.097</td>
  <td>0.168</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels, and use as many keypoints as the model returns. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>



















<tr>
  
  <td>SURF (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>7748.8</td>
  <td>0.208</td>
  <td>0.001</td>
  <td>0.006</td>
  <td>0.021</td>
  <td>0.054</td>
  <td>0.106</td>
 
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>


</tbody>
</table>



<hr></p>

<ul>
<li><a href="breakdown/phototourism/stereo/by_seq">Click here for a breakdown by sequence</a></li>
</ul>

<p><br /></p>

<h3 id="a-name-photot-mvs-a-task-p2-multi-view"><a name="photot_mvs"></a>Task P2: Multi-view</h3>

<p>Performance in SfM reconstruction, averaged over all the test sequences.</p>

<hr>

<p>









<table class="display leaderboard_mvs" width="100%" cellspacing="0">
<thead>
<tr>
  <th colspan="13" style="text-align: center;" class="sorter-false">MVS &mdash; averaged over all sequences</th>
</tr>
<tr>
  
  <th class="all">Method</th>
  <th class="all">Date</th>
  <th class="all">Type</th>
  
  <th class="all">Ims (%)</th>
  <th class="all">#Pts</th>
  <th class="all">SR</th>
  <th class="all">TL</th>
  <th class="all">mAP<sup>5<sup>o</sup></sup></th>
  <th class="all">mAP<sup>10<sup>o</sup></sup></th>
  <th class="all">mAP<sup>15<sup>o</sup></sup></th>
  <th class="all">mAP<sup>20<sup>o</sup></sup></th>
  <th class="all">mAP<sup>25<sup>o</sup></sup></th>
  <th class="all">ATE</th>
  
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
</tr>
</thead>
<tbody>































<tr>
  
  <td>AKAZE (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>79.3</td>
  <td>2163.6</td>
  <td>88.5</td>
  <td>2.86</td>
  <td>0.220</td>
  <td>0.306</td>
  <td>0.365</td>
  <td>0.413</td>
  <td>0.451</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td>ORB (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>72.8</td>
  <td>1432.0</td>
  <td>77.6</td>
  <td>2.59</td>
  <td>0.073</td>
  <td>0.121</td>
  <td>0.160</td>
  <td>0.196</td>
  <td>0.230</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td>GeoDesc (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>82.5</td>
  <td>3094.0</td>
  <td>95.8</td>
  <td>2.99</td>
  <td>0.325</td>
  <td>0.421</td>
  <td>0.480</td>
  <td>0.532</td>
  <td>0.571</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td>HardNet (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>83.2</td>
  <td>3788.6</td>
  <td>97.2</td>
  <td>2.97</td>
  <td>0.337</td>
  <td>0.439</td>
  <td>0.502</td>
  <td>0.552</td>
  <td>0.593</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td>L2-Net (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>82.5</td>
  <td>3313.0</td>
  <td>96.3</td>
  <td>2.90</td>
  <td>0.289</td>
  <td>0.386</td>
  <td>0.452</td>
  <td>0.503</td>
  <td>0.547</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td>SIFT (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>78.6</td>
  <td>2026.9</td>
  <td>88.5</td>
  <td>2.80</td>
  <td>0.214</td>
  <td>0.292</td>
  <td>0.347</td>
  <td>0.391</td>
  <td>0.429</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td>TFeat (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>81.7</td>
  <td>2825.6</td>
  <td>94.1</td>
  <td>2.84</td>
  <td>0.252</td>
  <td>0.346</td>
  <td>0.409</td>
  <td>0.462</td>
  <td>0.506</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td>SuperPoint</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>76.9</td>
  <td>570.6</td>
  <td>85.9</td>
  <td>3.03</td>
  <td>0.228</td>
  <td>0.319</td>
  <td>0.374</td>
  <td>0.417</td>
  <td>0.451</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels, and use as many keypoints as the model returns. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  































<tr>
  
  <td>SURF (8k, NN)</td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd">2019-04-24</td>
  <td>F</td>
  
  <td>74.9</td>
  <td>1158.8</td>
  <td>80.8</td>
  <td>2.60</td>
  <td>0.116</td>
  <td>0.177</td>
  <td>0.222</td>
  <td>0.260</td>
  <td>0.295</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
</tr>
  

</tbody>
</table>



<hr></p>

<ul>
<li><a href="breakdown/phototourism/mvs/by_seq">Click here for a breakdown by sequence</a></li>
<li><a href="breakdown/phototourism/mvs/by_bag">Click here for a breakdown by subset size</a></li>
</ul>

<p><br /><br /></p>

<h2 id="a-name-silda-a-dataset-2-silda-image-matching"><a name="silda"></a>Dataset 2: SILDa Image Matching</h2>

<p>Coming Soon.</p>

<p><br /><br /></p>

<h3 id="a-name-glossary-a-glossary"><a name="glossary"></a>Glossary</h3>

<p>Stereo task:

<ul>
  <li><b>Type:</b> Input format. "F": Features. "M": Features and matches. "P": Poses.</li>
  <li><b>#Kp:</b> Average number of keypoints extracted.</li>
  <li><b>MS:</b> Matching Score.</li>
  <li><b>mAP<sup>x</sup>:</b> Mean average precision at angular threshold *x*.</li>
</ul>

Multi-view reconstruction task:

<ul>
  <li><b>Type:</b> Input format. "F": Features. "M": Features and matches. "P": Poses.</li>
  <li><b>#Ims:</b> Ratio of images registered by the reconstruction (on successful reconstructions).</li>
  <li><b>#Pts:</b> Average number of 3D points obtained by the reconstruction.</li>
  <li><b>SR:</b> Success ratio (percentage) in the re-construction with COLMAP, out of 100 subsets.</li>
  <li><b>TL:</b> Average track length (number of 3D point observations).</li>
  <li><b>mAP<sup>x</sup>:</b> Mean average precision at angular threshold *x*.</li>
  <li><b>ATE:</b> Absolute Trajectory Error.</li>
</ul>
</p>

  </div>
</section>
<section id="tag-pane" class="row meta">
  
</section>








<section id="menu-pane" class="row menu text-center">
  
  
  
  
  
  <h4 class="text-center"><a class="menu-item" href="https://image-matching-workshop.github.io/">home</a></h4>
</section>



<footer class="row text-center footer">
  
  
</footer>

</div>




<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-62863910-6', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="js/main.js"></script>

</script>
</body>
</html>


