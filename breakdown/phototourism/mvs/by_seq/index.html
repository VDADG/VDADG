<!DOCTYPE html>
<html lang="en-US">
<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="description" content="Image Matching: Local Features &amp; Beyond - CVPR 2019 Workshop">
<meta name="keywords" content="image,matching,local Features, CVPR 2019, Workshop, CVPR">

<base href="https://image-matching-workshop.github.io/">

<title>Image Matching: Local Features &amp; Beyond</title>

<meta name="generator" content="Hugo 0.53" />



<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,400|Roboto+Slab:400,700|Roboto:300,300i,400,400i,500,500i,700,700i">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/all.css">
<link rel="stylesheet" href="https://image-matching-workshop.github.io/css/datatables.min.css">
<link rel="stylesheet" href="https://image-matching-workshop.github.io/css/balloon.min.css">
<link rel="stylesheet" href="https://image-matching-workshop.github.io/css/main.css">
<link rel="stylesheet" href="https://image-matching-workshop.github.io/css/custom.css">




<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="theme-color" content="#ffffff">





<script src="https://www.gstatic.com/charts/loader.js"></script>
<script type="text/javascript" src="https://image-matching-workshop.github.io/js/jquery.latest.min.js"></script>
<script type="text/javascript" src="https://image-matching-workshop.github.io/js/jquery.csv.min.js"></script>
<script type="text/javascript" src="https://image-matching-workshop.github.io/js/datatables.min.js"></script>
<script type="text/javascript" src="https://image-matching-workshop.github.io/js/three.js"></script>
<script type="text/javascript" src="https://image-matching-workshop.github.io/js/PCDLoader.js"></script>
<script type="text/javascript" src="https://image-matching-workshop.github.io/js/TrackballControls.js"></script>
<script type="text/javascript" src="https://image-matching-workshop.github.io/js/WebGL.js"></script>
<script type="text/javascript" src="https://image-matching-workshop.github.io/js/stats.min.js"></script>
<script>
$(document).ready(function (){
    var table = $('.leaderboard_stereo').DataTable({
        "columnDefs": [{targets:[0, 2], orderSequence: ['desc', 'asc']}],
        "columnDefs": [{targets:[1, 3, 4, 5, 6, 7, 8, 9], orderSequence: ['desc', 'asc']}],
        "order": [[7, 'desc']],
        responsive: {
            details: {
                renderer: function ( api, rowIdx, columns ) {
                    var data = $.map( columns, function ( col, i ) {
                        return col.hidden ?
                            '<tr data-dt-row="'+col.rowIndex+'" data-dt-column="'+col.columnIndex+'">'+
                                '<td class="align_left details_title"><b>'+col.title+':'+'</b></td> '+
                                '<td class="align_left details_data">'+col.data+'</td>'+
                            '</tr>' :
                            '';
                    } ).join('');
 
                    return data ?
                        $('<table/>').append( data ) :
                        false;
                }
            }
        }
    });

    var table = $('.leaderboard_mvs').DataTable({
        "columnDefs": [{targets:[0, 2], orderSequence: ['desc', 'asc']}],
        "columnDefs": [{targets:[1, 3, 4, 5, 6, 7, 8, 9, 10, 11], orderSequence: ['desc', 'asc']}],
        "order": [[9, 'desc']],
        responsive: {
            details: {
                renderer: function ( api, rowIdx, columns ) {
                    var data = $.map( columns, function ( col, i ) {
                        return col.hidden ?
                            '<tr data-dt-row="'+col.rowIndex+'" data-dt-column="'+col.columnIndex+'">'+
                                '<td class="align_left details_title"><b>'+col.title+':'+'</b></td> '+
                                '<td class="align_left details_data">'+col.data+'</td>'+
                            '</tr>' :
                            '';
                    } ).join('');
 
                    return data ?
                        $('<table/>').append( data ) :
                        false;
                }
            }
        }
    });

    var table = $('.checkerboard').DataTable({
        "columnDefs": [{targets:[0], orderSequence: ['desc', 'asc']}],
        "columnDefs": [{targets:[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], orderSequence: ['desc', 'asc']}],
        "order": [[12, 'desc']],
        responsive: {
            details: {
                renderer: function ( api, rowIdx, columns ) {
                    var data = $.map( columns, function ( col, i ) {
                        return col.hidden ?
                            '<tr data-dt-row="'+col.rowIndex+'" data-dt-column="'+col.columnIndex+'">'+
                                '<td class="align_left details_title"><b>'+col.title+':'+'</b></td> '+
                                '<td class="align_left details_data">'+col.data+'</td>'+
                            '</tr>' :
                            '';
                    } ).join('');
 
                    return data ?
                        $('<table/>').append( data ) :
                        false;
                }
            }
        }
    });

    $('.tablelist').dataTable({searching: false, paging: false, info: false});

    

    
    $('#btn-show-all-children').on('click', function(){
        
        table.rows(':not(.parent)').nodes().to$().find('td:first-child').trigger('click');
    });

    
    $('#btn-hide-all-children').on('click', function(){
        
        table.rows('.parent').nodes().to$().find('td:first-child').trigger('click');
    });
});
</script>
</head>
<body lang="en-US">
<div class="container">


<header class="row text-left title">
  <h1 class="title">Breakdown: Phototourism | MVS | Sequences</h1>
</header>
<section id="category-pane" class="row meta">
  
</section>
<section id="content-pane" class="row">
  <div class="col-md-12 text-justify content">
    <p>Breakdown on the Phototourism dataset, multi-view stereo task, by sequence.</p>

<ul>
<li><a href="leaderboard">Back to the leaderboard</a></li>
</ul>


<div id="chart-mvs-map15-perseq" style="width: 100%; height: 650px; margin-top: 0em">
<script type="text/javascript">
google.charts.load('current', {
  callback: function () {
    csvFile = "https:\/\/image-matching-workshop.github.io\/csv\/mvs-map15-perseq.csv";
    $.get(csvFile, function(csvString) {
      var arrayData = $.csv.toArrays(csvString, {onParseValue: $.csv.hooks.castToScalar});
      var data = new google.visualization.arrayToDataTable(arrayData);
      var chart = new google.visualization.LineChart(document.getElementById('chart-mvs-map15-perseq'));
      var options = {
        title: 'mAP at 15 degrees, per sequence',
        titleTextStyle: {
          color: '#aaa',
          fontName: 'Helvetica',
          fontSize: 18,
          bold: false,
        },
        legend: {
          position: 'right',
          textStyle: { fontSize: 12 },
          alignment: 'center',
        },
        chartArea: {
          left: '6%',
          right: '30%',
          bottom: '10%',
          width: "100%",
          height: "80%",
        },
        pointShape: 'circle',
        pointSize: 8,
        lineWidth: 0,
        backgroundColor: { fill:'transparent' },
        vAxis: {
          title: "mAP",
        },
        hAxis: {
          title: "Sequence",
        },
      };
    chart.draw(data, options);
    });
  },
  packages: ['corechart']
});
</script>
</div>



<p>















<br />
<br />
<table class="display checkerboard" width="99%" cellspacing="0">
<thead>
<tr>
  <th colspan="13" style="text-align: center;" class="sorter-false">MVS &mdash; All sequences &mdash; Sorted by mAP<sup>15<sup>o</sup></sup></th>
</tr>
<tr>
  
  <th class="all">Method</th>
  

<th class="all"><span class="balloon" data-balloon="british_museum" data-balloon-pos="up">BM</span></th>

<th class="all"><span class="balloon" data-balloon="florence_cathedral_side" data-balloon-pos="up">FCS</span></th>

<th class="all"><span class="balloon" data-balloon="lincoln_memorial_statue" data-balloon-pos="up">LMS</span></th>

<th class="all"><span class="balloon" data-balloon="london_bridge" data-balloon-pos="up">LB</span></th>

<th class="all"><span class="balloon" data-balloon="milan_cathedral" data-balloon-pos="up">MC</span></th>

<th class="all"><span class="balloon" data-balloon="mount_rushmore" data-balloon-pos="up">MR</span></th>

<th class="all"><span class="balloon" data-balloon="piazza_san_marco" data-balloon-pos="up">PSM</span></th>

<th class="all"><span class="balloon" data-balloon="reichstag" data-balloon-pos="up">RS</span></th>

<th class="all"><span class="balloon" data-balloon="sagrada_familia" data-balloon-pos="up">SF</span></th>

<th class="all"><span class="balloon" data-balloon="st_pauls_cathedral" data-balloon-pos="up">SPC</span></th>

<th class="all"><span class="balloon" data-balloon="united_states_capitol" data-balloon-pos="up">USC</span></th>

<th class="all"><span class="balloon" data-balloon="Average" data-balloon-pos="up">AVG</span></th>

  
  <th class="none hidden_key">Date</th>
  <th class="none hidden_key">Type</th>
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
  <th class="none hidden_key">Descriptor size</th>
</tr>
</thead>
<tbody>



<tr>
  
  <td class="align_left"><span class="boards_title">AKAZE (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.3576</td>










  <td>0.4385</td>










  <td>0.5551</td>










  <td>0.3696</td>










  <td>0.5935</td>










  <td>0.3131</td>










  <td>0.2543</td>










  <td>0.4708</td>










  <td>0.5709</td>










  <td>0.6054</td>










  <td>0.1847</td>










  <td>0.4285</td>

  
  <td class="hidden_value">19-04-24</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SphereDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.3772</td>










  <td>0.5865</td>










  <td>0.6595</td>










  <td>0.3789</td>










  <td>0.6131</td>










  <td>0.3458</td>










  <td>0.4122</td>










  <td>0.4310</td>










  <td>0.7381</td>










  <td>0.6481</td>










  <td>0.1606</td>










  <td>0.4865</td>

  
  <td class="hidden_value">19-04-26</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of AKAZE detector, and for each keypoint we extract a descriptor via CNN.</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">Brisk &#43; SSS</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.0844</td>










  <td>0.3523</td>










  <td>0.1150</td>










  <td>0.3040</td>










  <td>0.3429</td>










  <td>0.1207</td>










  <td>0.2678</td>










  <td>0.3511</td>










  <td>0.4466</td>










  <td>0.4849</td>










  <td>0.0936</td>










  <td>0.2694</td>

  
  <td class="hidden_value">19-05-14</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of brisk detector with the default settings, and for each image there are at most 8K keypoints. For each keypoint, we extract a descriptor via a CNN model.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (single scale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.2518</td>










  <td>0.4914</td>










  <td>0.6187</td>










  <td>0.4915</td>










  <td>0.4482</td>










  <td>0.3186</td>










  <td>0.3673</td>










  <td>0.3340</td>










  <td>0.5189</td>










  <td>0.5070</td>










  <td>0.1321</td>










  <td>0.4072</td>

  
  <td class="hidden_value">19-05-07</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Single-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (multiscale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.1904</td>










  <td>0.4486</td>










  <td>0.6308</td>










  <td>0.4622</td>










  <td>0.4263</td>










  <td>0.3543</td>










  <td>0.3746</td>










  <td>0.3690</td>










  <td>0.5083</td>










  <td>0.5044</td>










  <td>0.1323</td>










  <td>0.4001</td>

  
  <td class="hidden_value">19-05-07</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Multi-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>










  <td>0.0858</td>










  <td>0.2017</td>










  <td>0.2292</td>










  <td>0.1254</td>










  <td>0.1748</td>










  <td>0.0330</td>










  <td>0.0476</td>










  <td>0.1275</td>










  <td>0.0621</td>










  <td>0.2125</td>










  <td>0.0602</td>










  <td>0.1236</td>

  
  <td class="hidden_value">19-05-05</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>










  <td>0.1014</td>










  <td>0.2859</td>










  <td>0.2730</td>










  <td>0.1300</td>










  <td>0.2152</td>










  <td>0.0592</td>










  <td>0.1378</td>










  <td>0.1566</td>










  <td>0.1365</td>










  <td>0.2347</td>










  <td>0.0615</td>










  <td>0.1629</td>

  
  <td class="hidden_value">19-05-05</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>










  <td>0.0001</td>










  <td>0.0019</td>










  <td>0.0148</td>










  <td>0.0195</td>










  <td>0.0075</td>










  <td>0.0010</td>










  <td>%!f(int64=0000)</td>










  <td>0.0223</td>










  <td>%!f(int64=0000)</td>










  <td>0.0005</td>










  <td>0.0112</td>










  <td>0.0072</td>

  
  <td class="hidden_value">19-05-05</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>










  <td>0.0409</td>










  <td>0.0558</td>










  <td>0.0850</td>










  <td>0.0988</td>










  <td>0.0773</td>










  <td>0.0140</td>










  <td>0.0010</td>










  <td>0.0895</td>










  <td>0.0018</td>










  <td>0.1425</td>










  <td>0.0375</td>










  <td>0.0585</td>

  
  <td class="hidden_value">19-05-05</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">ELF-256D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>










  <td>0.0449</td>










  <td>0.0499</td>










  <td>0.1188</td>










  <td>0.1584</td>










  <td>0.0372</td>










  <td>0.0190</td>










  <td>0.0010</td>










  <td>0.1209</td>










  <td>0.0985</td>










  <td>0.0742</td>










  <td>0.0002</td>










  <td>0.0657</td>

  
  <td class="hidden_value">19-05-07</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">ELF-512D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>










  <td>0.0499</td>










  <td>0.0654</td>










  <td>0.1666</td>










  <td>0.2027</td>










  <td>0.0645</td>










  <td>0.0207</td>










  <td>0.0024</td>










  <td>0.1424</td>










  <td>0.0889</td>










  <td>0.1297</td>










  <td>0.0009</td>










  <td>0.0849</td>

  
  <td class="hidden_value">19-05-09</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">ELF-SIFT</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>










  <td>0.0933</td>










  <td>0.1873</td>










  <td>0.2702</td>










  <td>0.2906</td>










  <td>0.2627</td>










  <td>0.0799</td>










  <td>0.0148</td>










  <td>0.2254</td>










  <td>0.2236</td>










  <td>0.1925</td>










  <td>0.0055</td>










  <td>0.1678</td>

  
  <td class="hidden_value">19-04-26</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are HOG (as in SIFT).</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">ORB (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.1587</td>










  <td>0.3061</td>










  <td>0.3016</td>










  <td>0.1642</td>










  <td>0.2027</td>










  <td>0.1402</td>










  <td>0.1302</td>










  <td>0.3006</td>










  <td>0.3802</td>










  <td>0.3913</td>










  <td>0.0613</td>










  <td>0.2307</td>

  
  <td class="hidden_value">19-04-24</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (NN matcher)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.1532</td>










  <td>0.3674</td>










  <td>0.2183</td>










  <td>0.2667</td>










  <td>0.4313</td>










  <td>0.1428</td>










  <td>0.1834</td>










  <td>0.3325</td>










  <td>0.4663</td>










  <td>0.3444</td>










  <td>0.0502</td>










  <td>0.2688</td>

  
  <td class="hidden_value">19-05-10</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (custom matcher)</span><br /><span class="boards_subtitle">kp:8000, match:sift-aid<span></td>










  <td>0.1351</td>










  <td>0.3428</td>










  <td>0.1588</td>










  <td>0.2417</td>










  <td>0.4441</td>










  <td>0.1367</td>










  <td>0.1903</td>










  <td>0.3177</td>










  <td>0.4562</td>










  <td>0.3357</td>










  <td>0.0491</td>










  <td>0.2553</td>

  
  <td class="hidden_value">19-04-29</td>
  <td class="hidden_value">F/M</td>
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-ContextDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.3339</td>










  <td>0.6322</td>










  <td>0.7204</td>










  <td>0.5712</td>










  <td>0.7115</td>










  <td>0.4788</td>










  <td>0.4852</td>










  <td>0.4540</td>










  <td>0.7826</td>










  <td>0.5961</td>










  <td>0.1735</td>










  <td>0.5399</td>

  
  <td class="hidden_value">19-05-09</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">ContextDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features are quantized to uint8 and extracted from the code provided by the authors.</td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-GeoDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.3375</td>










  <td>0.6042</td>










  <td>0.6834</td>










  <td>0.4911</td>










  <td>0.7246</td>










  <td>0.4662</td>










  <td>0.4772</td>










  <td>0.4788</td>










  <td>0.7983</td>










  <td>0.6165</td>










  <td>0.1497</td>










  <td>0.5298</td>

  
  <td class="hidden_value">19-05-08</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features extracted from the code provided by the authors.</td>
  <td class="hidden_value"><a href="https://arxiv.org/abs/1807.06294">https://arxiv.org/abs/1807.06294</a></td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; GeoDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.4169</td>










  <td>0.6473</td>










  <td>0.6744</td>










  <td>0.4696</td>










  <td>0.7138</td>










  <td>0.4907</td>










  <td>0.4137</td>










  <td>0.4632</td>










  <td>0.7740</td>










  <td>0.6234</td>










  <td>0.1616</td>










  <td>0.5317</td>

  
  <td class="hidden_value">19-04-24</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; HardNet</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.4090</td>










  <td>0.6438</td>










  <td>0.7726</td>










  <td>0.4702</td>










  <td>0.6945</td>










  <td>0.5106</td>










  <td>0.4612</td>










  <td>0.4477</td>










  <td>0.8030</td>










  <td>0.6376</td>










  <td>0.1794</td>










  <td>0.5481</td>

  
  <td class="hidden_value">19-04-24</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; L2-Net</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.3292</td>










  <td>0.5935</td>










  <td>0.7109</td>










  <td>0.4380</td>










  <td>0.7093</td>










  <td>0.4667</td>










  <td>0.4000</td>










  <td>0.4192</td>










  <td>0.7682</td>










  <td>0.5832</td>










  <td>0.1778</td>










  <td>0.5087</td>

  
  <td class="hidden_value">19-04-24</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SIFT (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.2714</td>










  <td>0.5530</td>










  <td>0.4768</td>










  <td>0.3859</td>










  <td>0.6433</td>










  <td>0.3181</td>










  <td>0.2797</td>










  <td>0.4216</td>










  <td>0.6002</td>










  <td>0.4979</td>










  <td>0.1131</td>










  <td>0.4146</td>

  
  <td class="hidden_value">19-04-24</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; TFeat</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.3304</td>










  <td>0.5315</td>










  <td>0.6238</td>










  <td>0.4131</td>










  <td>0.6819</td>










  <td>0.4022</td>










  <td>0.3557</td>










  <td>0.3929</td>










  <td>0.6857</td>










  <td>0.5503</td>










  <td>0.1402</td>










  <td>0.4643</td>

  
  <td class="hidden_value">19-04-24</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint (default)</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>










  <td>0.3763</td>










  <td>0.3927</td>










  <td>0.6357</td>










  <td>0.5936</td>










  <td>0.5046</td>










  <td>0.3569</td>










  <td>0.3163</td>










  <td>0.3722</td>










  <td>0.4853</td>










  <td>0.5128</td>










  <td>0.1470</td>










  <td>0.4267</td>

  
  <td class="hidden_value">19-04-24</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We extract features with the default parameters and use however many are returned (about 1200 on average). Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>










  <td>0.3752</td>










  <td>0.3704</td>










  <td>0.6258</td>










  <td>0.6135</td>










  <td>0.4946</td>










  <td>0.3670</td>










  <td>0.2659</td>










  <td>0.3370</td>










  <td>0.4623</td>










  <td>0.5174</td>










  <td>0.1302</td>










  <td>0.4145</td>

  
  <td class="hidden_value">19-04-26</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>










  <td>0.3557</td>










  <td>0.3989</td>










  <td>0.5727</td>










  <td>0.5711</td>










  <td>0.5072</td>










  <td>0.3859</td>










  <td>0.3269</td>










  <td>0.3598</td>










  <td>0.4956</td>










  <td>0.5199</td>










  <td>0.1501</td>










  <td>0.4222</td>

  
  <td class="hidden_value">19-04-26</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>










  <td>0.1089</td>










  <td>0.0256</td>










  <td>0.2964</td>










  <td>0.3699</td>










  <td>0.0160</td>










  <td>0.1125</td>










  <td>0.0002</td>










  <td>0.1210</td>










  <td>0.2019</td>










  <td>0.1638</td>










  <td>0.0016</td>










  <td>0.1289</td>

  
  <td class="hidden_value">19-04-26</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>










  <td>0.3242</td>










  <td>0.1806</td>










  <td>0.5745</td>










  <td>0.5669</td>










  <td>0.3108</td>










  <td>0.2704</td>










  <td>0.0990</td>










  <td>0.2769</td>










  <td>0.3634</td>










  <td>0.4048</td>










  <td>0.0585</td>










  <td>0.3118</td>

  
  <td class="hidden_value">19-04-26</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.3283</td>










  <td>0.3907</td>










  <td>0.5753</td>










  <td>0.4620</td>










  <td>0.4401</td>










  <td>0.3203</td>










  <td>0.3695</td>










  <td>0.3651</td>










  <td>0.4828</td>










  <td>0.4707</td>










  <td>0.1208</td>










  <td>0.3932</td>

  
  <td class="hidden_value">19-04-26</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>




<tr>
  
  <td class="align_left"><span class="boards_title">SURF (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>










  <td>0.1458</td>










  <td>0.3515</td>










  <td>0.3862</td>










  <td>0.2207</td>










  <td>0.4503</td>










  <td>0.2126</td>










  <td>0.2072</td>










  <td>0.3438</td>










  <td>0.4977</td>










  <td>0.4159</td>










  <td>0.0760</td>










  <td>0.3007</td>

  
  <td class="hidden_value">19-04-24</td>
  <td class="hidden_value">F</td>
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>


</tbody>
</table>



<hr></p>

<p>Results for individual sequences:</p>

<ul>
<li><a href="breakdown/phototourism/mvs/by_seq#bm">British Museum</a></li>
<li><a href="breakdown/phototourism/mvs/by_seq#fcs">Florence Cathedral Side</a></li>
<li><a href="breakdown/phototourism/mvs/by_seq#lms">Lincoln Memorial Statue</a></li>
<li><a href="breakdown/phototourism/mvs/by_seq#lb">London Bridge</a></li>
<li><a href="breakdown/phototourism/mvs/by_seq#mc">Milan Cathedral</a></li>
<li><a href="breakdown/phototourism/mvs/by_seq#mr">Mount Rushmore</a></li>
<li><a href="breakdown/phototourism/mvs/by_seq#psm">Piazza San Marco</a></li>
<li><a href="breakdown/phototourism/mvs/by_seq#rs">Reichstag</a></li>
<li><a href="breakdown/phototourism/mvs/by_seq#sf">Sagrada Familia</a></li>
<li><a href="breakdown/phototourism/mvs/by_seq#spc">St. Paul&rsquo;s Cathedral</a></li>
<li><a href="breakdown/phototourism/mvs/by_seq#usc">United States Capitol</a></li>
</ul>

<p><br /></p>

<p><a name="bm"></a>










<table class="display leaderboard_mvs" width="99%" cellspacing="0">
<thead>
<tr>
  <th colspan="13" style="text-align: center;" class="sorter-false">MVS &mdash; sequence &#39;british_museum&#39;</th>
</tr>
<tr>
  
  <th class="all align_left" width="99%">Method</th>
  <th class="all">Date</th>
  <th class="all"><span class="balloon" data-balloon="F: Features only.&#10;M: Features and custom matches." data-balloon-break data-balloon-pos="up">Type</span></th>
  
  <th class="all"><span class="balloon" data-balloon="Ratio of images successfully&#10;registered (min: 3)." data-balloon-break data-balloon-pos="up">Ims (%)</span></th>
  <th class="all"><span class="balloon" data-balloon="Average number of 3D points&#10;in the reconstruction." data-balloon-break data-balloon-pos="up">#Pts</span></th>
  <th class="all"><span class="balloon" data-balloon="Success ratio in the&#10;3D reconstruction (%)." data-balloon-break data-balloon-pos="up">SR</span></th>
  <th class="all"><span class="balloon" data-balloon="Average Track Length&#10;(# of observations per 3D point)." data-balloon-break data-balloon-pos="up">TL</span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 5-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>5<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 10-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>10<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 15-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>15<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 20-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>20<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 25-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>25<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Average Trajectory Error." data-balloon-break data-balloon-pos="up">ATE</span></th>
  
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
  <th class="none hidden_key">Descriptor size</th>
</tr>
</thead>
<tbody>































<tr>
  
  <td class="align_left"><span class="boards_title">AKAZE (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>99.1</td>
  <td>4960.4</td>
  <td>99.5</td>
  <td>3.76</td>
  <td>0.1442</td>
  <td>0.2674</td>
  <td>0.3576</td>
  <td>0.4264</td>
  <td>0.4881</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SphereDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>99.4</td>
  <td>3516.7</td>
  <td>99.2</td>
  <td>3.30</td>
  <td>0.1580</td>
  <td>0.2773</td>
  <td>0.3772</td>
  <td>0.4609</td>
  <td>0.5263</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of AKAZE detector, and for each keypoint we extract a descriptor via CNN.</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">Brisk &#43; SSS</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-14</span></td>
  <td>F</td>
  
  <td>98.9</td>
  <td>2095.0</td>
  <td>98.0</td>
  <td>3.06</td>
  <td>0.0102</td>
  <td>0.0410</td>
  <td>0.0844</td>
  <td>0.1457</td>
  <td>0.2142</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of brisk detector with the default settings, and for each image there are at most 8K keypoints. For each keypoint, we extract a descriptor via a CNN model.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (single scale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>95.8</td>
  <td>3457.5</td>
  <td>94.5</td>
  <td>2.89</td>
  <td>0.0700</td>
  <td>0.1689</td>
  <td>0.2518</td>
  <td>0.3336</td>
  <td>0.4026</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Single-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (multiscale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>96.3</td>
  <td>3622.8</td>
  <td>95.5</td>
  <td>2.85</td>
  <td>0.0386</td>
  <td>0.1086</td>
  <td>0.1904</td>
  <td>0.2725</td>
  <td>0.3439</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Multi-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>91.7</td>
  <td>804.0</td>
  <td>89.5</td>
  <td>2.33</td>
  <td>0.0149</td>
  <td>0.0464</td>
  <td>0.0858</td>
  <td>0.1385</td>
  <td>0.1972</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>94.2</td>
  <td>1517.5</td>
  <td>92.2</td>
  <td>2.29</td>
  <td>0.0165</td>
  <td>0.0519</td>
  <td>0.1014</td>
  <td>0.1625</td>
  <td>0.2240</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>25.9</td>
  <td>55.0</td>
  <td>7.0</td>
  <td>1.76</td>
  <td>0.0000</td>
  <td>0.0001</td>
  <td>0.0001</td>
  <td>0.0002</td>
  <td>0.0004</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>79.2</td>
  <td>351.5</td>
  <td>70.0</td>
  <td>2.30</td>
  <td>0.0044</td>
  <td>0.0191</td>
  <td>0.0409</td>
  <td>0.0704</td>
  <td>0.1062</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-256D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>82.1</td>
  <td>277.2</td>
  <td>66.8</td>
  <td>2.73</td>
  <td>0.0055</td>
  <td>0.0210</td>
  <td>0.0449</td>
  <td>0.0785</td>
  <td>0.1183</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-512D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>81.4</td>
  <td>267.9</td>
  <td>73.8</td>
  <td>2.82</td>
  <td>0.0077</td>
  <td>0.0258</td>
  <td>0.0499</td>
  <td>0.0899</td>
  <td>0.1269</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-SIFT</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>87.6</td>
  <td>294.0</td>
  <td>70.0</td>
  <td>2.90</td>
  <td>0.0169</td>
  <td>0.0540</td>
  <td>0.0933</td>
  <td>0.1398</td>
  <td>0.1870</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are HOG (as in SIFT).</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ORB (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>97.5</td>
  <td>3255.1</td>
  <td>94.0</td>
  <td>3.39</td>
  <td>0.0334</td>
  <td>0.0898</td>
  <td>0.1587</td>
  <td>0.2268</td>
  <td>0.2955</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (NN matcher)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-10</span></td>
  <td>F</td>
  
  <td>97.6</td>
  <td>1898.7</td>
  <td>92.0</td>
  <td>3.02</td>
  <td>0.0354</td>
  <td>0.0877</td>
  <td>0.1532</td>
  <td>0.2208</td>
  <td>0.2907</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (custom matcher)</span><br /><span class="boards_subtitle">kp:8000, match:sift-aid<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-29</span></td>
  <td>F/M</td>
  
  <td>96.4</td>
  <td>1816.7</td>
  <td>92.8</td>
  <td>2.95</td>
  <td>0.0275</td>
  <td>0.0754</td>
  <td>0.1351</td>
  <td>0.1993</td>
  <td>0.2716</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-ContextDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>99.3</td>
  <td>6043.3</td>
  <td>99.8</td>
  <td>2.92</td>
  <td>0.1531</td>
  <td>0.2511</td>
  <td>0.3339</td>
  <td>0.4077</td>
  <td>0.4683</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">ContextDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features are quantized to uint8 and extracted from the code provided by the authors.</td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-GeoDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-08</span></td>
  <td>F</td>
  
  <td>99.4</td>
  <td>4407.3</td>
  <td>99.2</td>
  <td>3.14</td>
  <td>0.1621</td>
  <td>0.2620</td>
  <td>0.3375</td>
  <td>0.4073</td>
  <td>0.4742</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features extracted from the code provided by the authors.</td>
  <td class="hidden_value"><a href="https://arxiv.org/abs/1807.06294">https://arxiv.org/abs/1807.06294</a></td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; GeoDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>99.5</td>
  <td>5722.5</td>
  <td>100.0</td>
  <td>3.45</td>
  <td>0.2066</td>
  <td>0.3316</td>
  <td>0.4169</td>
  <td>0.4933</td>
  <td>0.5506</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; HardNet</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>99.6</td>
  <td>6837.1</td>
  <td>100.0</td>
  <td>3.38</td>
  <td>0.2024</td>
  <td>0.3305</td>
  <td>0.4090</td>
  <td>0.4957</td>
  <td>0.5559</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; L2-Net</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>99.6</td>
  <td>6314.7</td>
  <td>100.0</td>
  <td>3.22</td>
  <td>0.1492</td>
  <td>0.2527</td>
  <td>0.3292</td>
  <td>0.4030</td>
  <td>0.4699</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>98.1</td>
  <td>4336.2</td>
  <td>97.2</td>
  <td>3.25</td>
  <td>0.1122</td>
  <td>0.2007</td>
  <td>0.2714</td>
  <td>0.3424</td>
  <td>0.4135</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; TFeat</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>99.1</td>
  <td>5718.0</td>
  <td>99.8</td>
  <td>3.20</td>
  <td>0.1452</td>
  <td>0.2462</td>
  <td>0.3304</td>
  <td>0.4033</td>
  <td>0.4724</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint (default)</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>95.6</td>
  <td>1003.0</td>
  <td>93.5</td>
  <td>3.33</td>
  <td>0.1745</td>
  <td>0.2897</td>
  <td>0.3763</td>
  <td>0.4470</td>
  <td>0.5024</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We extract features with the default parameters and use however many are returned (about 1200 on average). Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>95.9</td>
  <td>915.3</td>
  <td>93.5</td>
  <td>3.28</td>
  <td>0.1755</td>
  <td>0.2912</td>
  <td>0.3752</td>
  <td>0.4456</td>
  <td>0.4965</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>95.2</td>
  <td>1634.1</td>
  <td>94.8</td>
  <td>3.26</td>
  <td>0.1567</td>
  <td>0.2703</td>
  <td>0.3557</td>
  <td>0.4228</td>
  <td>0.4766</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>75.9</td>
  <td>173.6</td>
  <td>47.5</td>
  <td>2.92</td>
  <td>0.0402</td>
  <td>0.0793</td>
  <td>0.1089</td>
  <td>0.1323</td>
  <td>0.1505</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>93.2</td>
  <td>457.5</td>
  <td>87.5</td>
  <td>3.14</td>
  <td>0.1349</td>
  <td>0.2415</td>
  <td>0.3242</td>
  <td>0.3830</td>
  <td>0.4355</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>96.2</td>
  <td>5766.0</td>
  <td>94.5</td>
  <td>3.25</td>
  <td>0.1269</td>
  <td>0.2422</td>
  <td>0.3283</td>
  <td>0.3987</td>
  <td>0.4625</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SURF (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>97.8</td>
  <td>3199.3</td>
  <td>94.2</td>
  <td>2.89</td>
  <td>0.0386</td>
  <td>0.0920</td>
  <td>0.1458</td>
  <td>0.2112</td>
  <td>0.2830</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  

</tbody>
</table>



<hr>
<a name="fcs"></a>










<table class="display leaderboard_mvs" width="99%" cellspacing="0">
<thead>
<tr>
  <th colspan="13" style="text-align: center;" class="sorter-false">MVS &mdash; sequence &#39;florence_cathedral_side&#39;</th>
</tr>
<tr>
  
  <th class="all align_left" width="99%">Method</th>
  <th class="all">Date</th>
  <th class="all"><span class="balloon" data-balloon="F: Features only.&#10;M: Features and custom matches." data-balloon-break data-balloon-pos="up">Type</span></th>
  
  <th class="all"><span class="balloon" data-balloon="Ratio of images successfully&#10;registered (min: 3)." data-balloon-break data-balloon-pos="up">Ims (%)</span></th>
  <th class="all"><span class="balloon" data-balloon="Average number of 3D points&#10;in the reconstruction." data-balloon-break data-balloon-pos="up">#Pts</span></th>
  <th class="all"><span class="balloon" data-balloon="Success ratio in the&#10;3D reconstruction (%)." data-balloon-break data-balloon-pos="up">SR</span></th>
  <th class="all"><span class="balloon" data-balloon="Average Track Length&#10;(# of observations per 3D point)." data-balloon-break data-balloon-pos="up">TL</span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 5-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>5<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 10-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>10<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 15-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>15<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 20-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>20<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 25-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>25<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Average Trajectory Error." data-balloon-break data-balloon-pos="up">ATE</span></th>
  
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
  <th class="none hidden_key">Descriptor size</th>
</tr>
</thead>
<tbody>































<tr>
  
  <td class="align_left"><span class="boards_title">AKAZE (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>92.3</td>
  <td>4821.8</td>
  <td>81.7</td>
  <td>3.06</td>
  <td>0.3638</td>
  <td>0.4103</td>
  <td>0.4385</td>
  <td>0.4615</td>
  <td>0.4841</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SphereDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>97.8</td>
  <td>7748.9</td>
  <td>95.2</td>
  <td>3.16</td>
  <td>0.4741</td>
  <td>0.5381</td>
  <td>0.5865</td>
  <td>0.6261</td>
  <td>0.6657</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of AKAZE detector, and for each keypoint we extract a descriptor via CNN.</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">Brisk &#43; SSS</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-14</span></td>
  <td>F</td>
  
  <td>93.7</td>
  <td>3959.1</td>
  <td>87.2</td>
  <td>2.60</td>
  <td>0.2233</td>
  <td>0.2980</td>
  <td>0.3523</td>
  <td>0.4039</td>
  <td>0.4457</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of brisk detector with the default settings, and for each image there are at most 8K keypoints. For each keypoint, we extract a descriptor via a CNN model.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (single scale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>92.5</td>
  <td>4421.4</td>
  <td>91.5</td>
  <td>3.06</td>
  <td>0.3081</td>
  <td>0.4217</td>
  <td>0.4914</td>
  <td>0.5396</td>
  <td>0.5713</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Single-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (multiscale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>93.0</td>
  <td>6092.1</td>
  <td>93.2</td>
  <td>2.89</td>
  <td>0.2586</td>
  <td>0.3721</td>
  <td>0.4486</td>
  <td>0.5162</td>
  <td>0.5684</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Multi-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>85.3</td>
  <td>588.5</td>
  <td>74.0</td>
  <td>2.42</td>
  <td>0.0332</td>
  <td>0.1237</td>
  <td>0.2017</td>
  <td>0.2653</td>
  <td>0.3064</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>89.3</td>
  <td>1200.4</td>
  <td>81.2</td>
  <td>2.41</td>
  <td>0.0868</td>
  <td>0.2067</td>
  <td>0.2859</td>
  <td>0.3434</td>
  <td>0.3916</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>55.7</td>
  <td>76.4</td>
  <td>22.8</td>
  <td>2.26</td>
  <td>0.0000</td>
  <td>0.0005</td>
  <td>0.0019</td>
  <td>0.0044</td>
  <td>0.0074</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>72.9</td>
  <td>239.8</td>
  <td>57.2</td>
  <td>2.36</td>
  <td>0.0047</td>
  <td>0.0225</td>
  <td>0.0558</td>
  <td>0.0939</td>
  <td>0.1246</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-256D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>37.4</td>
  <td>179.8</td>
  <td>32.8</td>
  <td>1.87</td>
  <td>0.0302</td>
  <td>0.0436</td>
  <td>0.0499</td>
  <td>0.0542</td>
  <td>0.0581</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-512D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>64.3</td>
  <td>194.4</td>
  <td>40.2</td>
  <td>2.28</td>
  <td>0.0324</td>
  <td>0.0521</td>
  <td>0.0654</td>
  <td>0.0736</td>
  <td>0.0813</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-SIFT</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>75.3</td>
  <td>309.9</td>
  <td>47.2</td>
  <td>2.82</td>
  <td>0.1363</td>
  <td>0.1706</td>
  <td>0.1873</td>
  <td>0.1945</td>
  <td>0.2006</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are HOG (as in SIFT).</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ORB (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>87.7</td>
  <td>3858.0</td>
  <td>71.0</td>
  <td>2.64</td>
  <td>0.2375</td>
  <td>0.2792</td>
  <td>0.3061</td>
  <td>0.3266</td>
  <td>0.3509</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (NN matcher)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-10</span></td>
  <td>F</td>
  
  <td>89.4</td>
  <td>4132.4</td>
  <td>78.0</td>
  <td>2.76</td>
  <td>0.2820</td>
  <td>0.3401</td>
  <td>0.3674</td>
  <td>0.3938</td>
  <td>0.4214</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (custom matcher)</span><br /><span class="boards_subtitle">kp:8000, match:sift-aid<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-29</span></td>
  <td>F/M</td>
  
  <td>87.3</td>
  <td>3781.1</td>
  <td>74.8</td>
  <td>2.66</td>
  <td>0.2627</td>
  <td>0.3147</td>
  <td>0.3428</td>
  <td>0.3638</td>
  <td>0.3850</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-ContextDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>98.5</td>
  <td>7259.7</td>
  <td>96.0</td>
  <td>3.26</td>
  <td>0.5283</td>
  <td>0.5921</td>
  <td>0.6322</td>
  <td>0.6655</td>
  <td>0.6918</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">ContextDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features are quantized to uint8 and extracted from the code provided by the authors.</td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-GeoDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-08</span></td>
  <td>F</td>
  
  <td>98.0</td>
  <td>6651.5</td>
  <td>95.2</td>
  <td>3.32</td>
  <td>0.5159</td>
  <td>0.5732</td>
  <td>0.6042</td>
  <td>0.6374</td>
  <td>0.6741</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features extracted from the code provided by the authors.</td>
  <td class="hidden_value"><a href="https://arxiv.org/abs/1807.06294">https://arxiv.org/abs/1807.06294</a></td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; GeoDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>98.3</td>
  <td>7094.6</td>
  <td>95.8</td>
  <td>3.50</td>
  <td>0.5467</td>
  <td>0.6084</td>
  <td>0.6473</td>
  <td>0.6791</td>
  <td>0.7146</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; HardNet</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>98.8</td>
  <td>7774.4</td>
  <td>96.8</td>
  <td>3.46</td>
  <td>0.5471</td>
  <td>0.6084</td>
  <td>0.6438</td>
  <td>0.6724</td>
  <td>0.7028</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; L2-Net</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>98.0</td>
  <td>7250.3</td>
  <td>96.5</td>
  <td>3.33</td>
  <td>0.5012</td>
  <td>0.5587</td>
  <td>0.5935</td>
  <td>0.6245</td>
  <td>0.6627</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>96.7</td>
  <td>6040.3</td>
  <td>92.5</td>
  <td>3.31</td>
  <td>0.4774</td>
  <td>0.5243</td>
  <td>0.5530</td>
  <td>0.5861</td>
  <td>0.6184</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; TFeat</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>97.6</td>
  <td>6488.0</td>
  <td>95.0</td>
  <td>3.26</td>
  <td>0.4359</td>
  <td>0.4882</td>
  <td>0.5315</td>
  <td>0.5723</td>
  <td>0.6088</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint (default)</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>88.7</td>
  <td>1320.5</td>
  <td>80.5</td>
  <td>3.28</td>
  <td>0.3088</td>
  <td>0.3606</td>
  <td>0.3927</td>
  <td>0.4212</td>
  <td>0.4471</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We extract features with the default parameters and use however many are returned (about 1200 on average). Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>85.5</td>
  <td>850.7</td>
  <td>74.5</td>
  <td>3.13</td>
  <td>0.2989</td>
  <td>0.3428</td>
  <td>0.3704</td>
  <td>0.3905</td>
  <td>0.4057</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>89.5</td>
  <td>1528.8</td>
  <td>82.8</td>
  <td>3.29</td>
  <td>0.3072</td>
  <td>0.3622</td>
  <td>0.3989</td>
  <td>0.4326</td>
  <td>0.4630</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>17.9</td>
  <td>81.3</td>
  <td>20.5</td>
  <td>1.31</td>
  <td>0.0209</td>
  <td>0.0239</td>
  <td>0.0256</td>
  <td>0.0267</td>
  <td>0.0280</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>73.0</td>
  <td>355.0</td>
  <td>55.2</td>
  <td>2.75</td>
  <td>0.1514</td>
  <td>0.1694</td>
  <td>0.1806</td>
  <td>0.1922</td>
  <td>0.2005</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>91.0</td>
  <td>5043.3</td>
  <td>89.2</td>
  <td>3.10</td>
  <td>0.2632</td>
  <td>0.3433</td>
  <td>0.3907</td>
  <td>0.4282</td>
  <td>0.4653</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SURF (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>88.6</td>
  <td>3721.2</td>
  <td>72.2</td>
  <td>2.72</td>
  <td>0.2854</td>
  <td>0.3243</td>
  <td>0.3515</td>
  <td>0.3740</td>
  <td>0.3960</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  

</tbody>
</table>



<hr>
<a name="lms"></a>










<table class="display leaderboard_mvs" width="99%" cellspacing="0">
<thead>
<tr>
  <th colspan="13" style="text-align: center;" class="sorter-false">MVS &mdash; sequence &#39;lincoln_memorial_statue&#39;</th>
</tr>
<tr>
  
  <th class="all align_left" width="99%">Method</th>
  <th class="all">Date</th>
  <th class="all"><span class="balloon" data-balloon="F: Features only.&#10;M: Features and custom matches." data-balloon-break data-balloon-pos="up">Type</span></th>
  
  <th class="all"><span class="balloon" data-balloon="Ratio of images successfully&#10;registered (min: 3)." data-balloon-break data-balloon-pos="up">Ims (%)</span></th>
  <th class="all"><span class="balloon" data-balloon="Average number of 3D points&#10;in the reconstruction." data-balloon-break data-balloon-pos="up">#Pts</span></th>
  <th class="all"><span class="balloon" data-balloon="Success ratio in the&#10;3D reconstruction (%)." data-balloon-break data-balloon-pos="up">SR</span></th>
  <th class="all"><span class="balloon" data-balloon="Average Track Length&#10;(# of observations per 3D point)." data-balloon-break data-balloon-pos="up">TL</span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 5-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>5<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 10-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>10<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 15-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>15<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 20-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>20<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 25-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>25<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Average Trajectory Error." data-balloon-break data-balloon-pos="up">ATE</span></th>
  
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
  <th class="none hidden_key">Descriptor size</th>
</tr>
</thead>
<tbody>































<tr>
  
  <td class="align_left"><span class="boards_title">AKAZE (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>95.9</td>
  <td>5196.5</td>
  <td>87.5</td>
  <td>3.21</td>
  <td>0.4596</td>
  <td>0.5232</td>
  <td>0.5551</td>
  <td>0.5818</td>
  <td>0.6020</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SphereDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>97.6</td>
  <td>1076.8</td>
  <td>93.2</td>
  <td>4.25</td>
  <td>0.5305</td>
  <td>0.6167</td>
  <td>0.6595</td>
  <td>0.6920</td>
  <td>0.7132</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of AKAZE detector, and for each keypoint we extract a descriptor via CNN.</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">Brisk &#43; SSS</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-14</span></td>
  <td>F</td>
  
  <td>90.9</td>
  <td>793.1</td>
  <td>87.2</td>
  <td>3.12</td>
  <td>0.0365</td>
  <td>0.0796</td>
  <td>0.1150</td>
  <td>0.1471</td>
  <td>0.1801</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of brisk detector with the default settings, and for each image there are at most 8K keypoints. For each keypoint, we extract a descriptor via a CNN model.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (single scale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>94.3</td>
  <td>4540.1</td>
  <td>94.8</td>
  <td>3.05</td>
  <td>0.4675</td>
  <td>0.5678</td>
  <td>0.6187</td>
  <td>0.6494</td>
  <td>0.6767</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Single-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (multiscale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>95.0</td>
  <td>6023.8</td>
  <td>95.5</td>
  <td>2.94</td>
  <td>0.4699</td>
  <td>0.5761</td>
  <td>0.6308</td>
  <td>0.6671</td>
  <td>0.7006</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Multi-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>80.2</td>
  <td>534.1</td>
  <td>66.0</td>
  <td>2.38</td>
  <td>0.0837</td>
  <td>0.1753</td>
  <td>0.2292</td>
  <td>0.2669</td>
  <td>0.2874</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>84.3</td>
  <td>1020.8</td>
  <td>71.5</td>
  <td>2.35</td>
  <td>0.1008</td>
  <td>0.2074</td>
  <td>0.2730</td>
  <td>0.3139</td>
  <td>0.3425</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>32.0</td>
  <td>63.5</td>
  <td>31.2</td>
  <td>1.73</td>
  <td>0.0016</td>
  <td>0.0077</td>
  <td>0.0148</td>
  <td>0.0205</td>
  <td>0.0239</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>69.3</td>
  <td>201.4</td>
  <td>53.5</td>
  <td>2.34</td>
  <td>0.0196</td>
  <td>0.0570</td>
  <td>0.0850</td>
  <td>0.1064</td>
  <td>0.1212</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-256D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>69.3</td>
  <td>160.9</td>
  <td>46.0</td>
  <td>2.87</td>
  <td>0.0744</td>
  <td>0.1020</td>
  <td>0.1188</td>
  <td>0.1301</td>
  <td>0.1364</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-512D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>73.1</td>
  <td>195.6</td>
  <td>53.2</td>
  <td>2.93</td>
  <td>0.1044</td>
  <td>0.1440</td>
  <td>0.1666</td>
  <td>0.1778</td>
  <td>0.1867</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-SIFT</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>79.6</td>
  <td>329.7</td>
  <td>66.5</td>
  <td>3.09</td>
  <td>0.1974</td>
  <td>0.2482</td>
  <td>0.2702</td>
  <td>0.2898</td>
  <td>0.3026</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are HOG (as in SIFT).</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ORB (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>95.6</td>
  <td>2357.2</td>
  <td>91.8</td>
  <td>3.47</td>
  <td>0.2133</td>
  <td>0.2653</td>
  <td>0.3016</td>
  <td>0.3404</td>
  <td>0.3774</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (NN matcher)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-10</span></td>
  <td>F</td>
  
  <td>88.9</td>
  <td>820.2</td>
  <td>83.2</td>
  <td>3.42</td>
  <td>0.1204</td>
  <td>0.1771</td>
  <td>0.2183</td>
  <td>0.2457</td>
  <td>0.2696</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (custom matcher)</span><br /><span class="boards_subtitle">kp:8000, match:sift-aid<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-29</span></td>
  <td>F/M</td>
  
  <td>86.8</td>
  <td>725.6</td>
  <td>80.2</td>
  <td>3.21</td>
  <td>0.0821</td>
  <td>0.1254</td>
  <td>0.1588</td>
  <td>0.1892</td>
  <td>0.2132</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-ContextDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>98.8</td>
  <td>5106.3</td>
  <td>97.0</td>
  <td>3.43</td>
  <td>0.6233</td>
  <td>0.6881</td>
  <td>0.7204</td>
  <td>0.7408</td>
  <td>0.7649</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">ContextDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features are quantized to uint8 and extracted from the code provided by the authors.</td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-GeoDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-08</span></td>
  <td>F</td>
  
  <td>98.5</td>
  <td>4395.5</td>
  <td>96.8</td>
  <td>3.52</td>
  <td>0.5948</td>
  <td>0.6508</td>
  <td>0.6834</td>
  <td>0.7094</td>
  <td>0.7323</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features extracted from the code provided by the authors.</td>
  <td class="hidden_value"><a href="https://arxiv.org/abs/1807.06294">https://arxiv.org/abs/1807.06294</a></td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; GeoDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>98.1</td>
  <td>5615.2</td>
  <td>96.8</td>
  <td>3.55</td>
  <td>0.6031</td>
  <td>0.6511</td>
  <td>0.6744</td>
  <td>0.6970</td>
  <td>0.7121</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; HardNet</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>98.6</td>
  <td>6905.6</td>
  <td>96.8</td>
  <td>3.56</td>
  <td>0.6800</td>
  <td>0.7454</td>
  <td>0.7726</td>
  <td>0.7886</td>
  <td>0.8055</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; L2-Net</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>98.5</td>
  <td>6278.0</td>
  <td>94.2</td>
  <td>3.47</td>
  <td>0.6097</td>
  <td>0.6791</td>
  <td>0.7109</td>
  <td>0.7307</td>
  <td>0.7504</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>94.9</td>
  <td>3769.6</td>
  <td>86.8</td>
  <td>3.27</td>
  <td>0.3983</td>
  <td>0.4470</td>
  <td>0.4768</td>
  <td>0.5010</td>
  <td>0.5233</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; TFeat</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>98.0</td>
  <td>5567.0</td>
  <td>92.2</td>
  <td>3.37</td>
  <td>0.5373</td>
  <td>0.5954</td>
  <td>0.6238</td>
  <td>0.6453</td>
  <td>0.6680</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint (default)</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>93.6</td>
  <td>791.2</td>
  <td>87.2</td>
  <td>4.40</td>
  <td>0.5501</td>
  <td>0.6104</td>
  <td>0.6357</td>
  <td>0.6530</td>
  <td>0.6638</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We extract features with the default parameters and use however many are returned (about 1200 on average). Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>93.5</td>
  <td>967.9</td>
  <td>87.5</td>
  <td>4.26</td>
  <td>0.5339</td>
  <td>0.5940</td>
  <td>0.6258</td>
  <td>0.6436</td>
  <td>0.6547</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>93.1</td>
  <td>1772.5</td>
  <td>88.5</td>
  <td>3.71</td>
  <td>0.4758</td>
  <td>0.5381</td>
  <td>0.5727</td>
  <td>0.5973</td>
  <td>0.6115</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>84.0</td>
  <td>238.9</td>
  <td>79.5</td>
  <td>4.24</td>
  <td>0.2156</td>
  <td>0.2626</td>
  <td>0.2964</td>
  <td>0.3136</td>
  <td>0.3343</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>92.7</td>
  <td>519.7</td>
  <td>85.0</td>
  <td>4.58</td>
  <td>0.4984</td>
  <td>0.5515</td>
  <td>0.5745</td>
  <td>0.5930</td>
  <td>0.6051</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>94.4</td>
  <td>6401.4</td>
  <td>90.5</td>
  <td>3.23</td>
  <td>0.4430</td>
  <td>0.5314</td>
  <td>0.5753</td>
  <td>0.6083</td>
  <td>0.6304</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SURF (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>94.5</td>
  <td>3245.4</td>
  <td>85.0</td>
  <td>2.98</td>
  <td>0.3038</td>
  <td>0.3571</td>
  <td>0.3862</td>
  <td>0.4119</td>
  <td>0.4384</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  

</tbody>
</table>



<hr>
<a name="lb"></a>










<table class="display leaderboard_mvs" width="99%" cellspacing="0">
<thead>
<tr>
  <th colspan="13" style="text-align: center;" class="sorter-false">MVS &mdash; sequence &#39;london_bridge&#39;</th>
</tr>
<tr>
  
  <th class="all align_left" width="99%">Method</th>
  <th class="all">Date</th>
  <th class="all"><span class="balloon" data-balloon="F: Features only.&#10;M: Features and custom matches." data-balloon-break data-balloon-pos="up">Type</span></th>
  
  <th class="all"><span class="balloon" data-balloon="Ratio of images successfully&#10;registered (min: 3)." data-balloon-break data-balloon-pos="up">Ims (%)</span></th>
  <th class="all"><span class="balloon" data-balloon="Average number of 3D points&#10;in the reconstruction." data-balloon-break data-balloon-pos="up">#Pts</span></th>
  <th class="all"><span class="balloon" data-balloon="Success ratio in the&#10;3D reconstruction (%)." data-balloon-break data-balloon-pos="up">SR</span></th>
  <th class="all"><span class="balloon" data-balloon="Average Track Length&#10;(# of observations per 3D point)." data-balloon-break data-balloon-pos="up">TL</span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 5-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>5<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 10-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>10<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 15-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>15<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 20-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>20<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 25-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>25<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Average Trajectory Error." data-balloon-break data-balloon-pos="up">ATE</span></th>
  
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
  <th class="none hidden_key">Descriptor size</th>
</tr>
</thead>
<tbody>































<tr>
  
  <td class="align_left"><span class="boards_title">AKAZE (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>94.1</td>
  <td>3666.5</td>
  <td>89.2</td>
  <td>3.27</td>
  <td>0.2433</td>
  <td>0.3206</td>
  <td>0.3696</td>
  <td>0.4181</td>
  <td>0.4534</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SphereDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>96.8</td>
  <td>2633.4</td>
  <td>94.2</td>
  <td>3.19</td>
  <td>0.2211</td>
  <td>0.3162</td>
  <td>0.3789</td>
  <td>0.4291</td>
  <td>0.4686</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of AKAZE detector, and for each keypoint we extract a descriptor via CNN.</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">Brisk &#43; SSS</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-14</span></td>
  <td>F</td>
  
  <td>94.6</td>
  <td>2154.4</td>
  <td>89.8</td>
  <td>2.83</td>
  <td>0.1277</td>
  <td>0.2320</td>
  <td>0.3040</td>
  <td>0.3615</td>
  <td>0.4028</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of brisk detector with the default settings, and for each image there are at most 8K keypoints. For each keypoint, we extract a descriptor via a CNN model.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (single scale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>96.4</td>
  <td>3939.5</td>
  <td>97.0</td>
  <td>3.31</td>
  <td>0.2558</td>
  <td>0.4064</td>
  <td>0.4915</td>
  <td>0.5478</td>
  <td>0.5956</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Single-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (multiscale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>96.4</td>
  <td>5022.2</td>
  <td>97.0</td>
  <td>3.05</td>
  <td>0.2057</td>
  <td>0.3689</td>
  <td>0.4622</td>
  <td>0.5308</td>
  <td>0.5852</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Multi-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>92.2</td>
  <td>922.8</td>
  <td>91.5</td>
  <td>2.50</td>
  <td>0.0184</td>
  <td>0.0747</td>
  <td>0.1254</td>
  <td>0.1797</td>
  <td>0.2241</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>94.0</td>
  <td>1667.0</td>
  <td>94.8</td>
  <td>2.48</td>
  <td>0.0207</td>
  <td>0.0800</td>
  <td>0.1300</td>
  <td>0.1810</td>
  <td>0.2237</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>60.8</td>
  <td>109.8</td>
  <td>39.0</td>
  <td>2.29</td>
  <td>0.0027</td>
  <td>0.0104</td>
  <td>0.0195</td>
  <td>0.0277</td>
  <td>0.0360</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>83.5</td>
  <td>432.5</td>
  <td>77.5</td>
  <td>2.48</td>
  <td>0.0154</td>
  <td>0.0543</td>
  <td>0.0988</td>
  <td>0.1371</td>
  <td>0.1746</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-256D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>73.9</td>
  <td>159.8</td>
  <td>46.8</td>
  <td>2.97</td>
  <td>0.0904</td>
  <td>0.1372</td>
  <td>0.1584</td>
  <td>0.1737</td>
  <td>0.1847</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-512D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>77.7</td>
  <td>169.0</td>
  <td>55.8</td>
  <td>3.10</td>
  <td>0.0979</td>
  <td>0.1649</td>
  <td>0.2027</td>
  <td>0.2218</td>
  <td>0.2363</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-SIFT</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>84.8</td>
  <td>328.1</td>
  <td>65.5</td>
  <td>3.12</td>
  <td>0.1809</td>
  <td>0.2550</td>
  <td>0.2906</td>
  <td>0.3187</td>
  <td>0.3396</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are HOG (as in SIFT).</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ORB (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>88.4</td>
  <td>2595.0</td>
  <td>78.5</td>
  <td>2.82</td>
  <td>0.0736</td>
  <td>0.1280</td>
  <td>0.1642</td>
  <td>0.1981</td>
  <td>0.2300</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (NN matcher)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-10</span></td>
  <td>F</td>
  
  <td>94.2</td>
  <td>2102.2</td>
  <td>89.5</td>
  <td>3.02</td>
  <td>0.1235</td>
  <td>0.2106</td>
  <td>0.2667</td>
  <td>0.3153</td>
  <td>0.3640</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (custom matcher)</span><br /><span class="boards_subtitle">kp:8000, match:sift-aid<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-29</span></td>
  <td>F/M</td>
  
  <td>93.6</td>
  <td>2021.3</td>
  <td>88.8</td>
  <td>2.93</td>
  <td>0.1019</td>
  <td>0.1855</td>
  <td>0.2417</td>
  <td>0.2926</td>
  <td>0.3357</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-ContextDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>98.2</td>
  <td>5949.2</td>
  <td>98.0</td>
  <td>3.28</td>
  <td>0.4002</td>
  <td>0.5159</td>
  <td>0.5712</td>
  <td>0.6138</td>
  <td>0.6520</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">ContextDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features are quantized to uint8 and extracted from the code provided by the authors.</td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-GeoDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-08</span></td>
  <td>F</td>
  
  <td>98.1</td>
  <td>4628.8</td>
  <td>97.5</td>
  <td>3.39</td>
  <td>0.3420</td>
  <td>0.4375</td>
  <td>0.4911</td>
  <td>0.5313</td>
  <td>0.5713</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features extracted from the code provided by the authors.</td>
  <td class="hidden_value"><a href="https://arxiv.org/abs/1807.06294">https://arxiv.org/abs/1807.06294</a></td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; GeoDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>97.3</td>
  <td>4744.3</td>
  <td>97.0</td>
  <td>3.39</td>
  <td>0.3242</td>
  <td>0.4196</td>
  <td>0.4696</td>
  <td>0.5165</td>
  <td>0.5491</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; HardNet</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>98.3</td>
  <td>6062.4</td>
  <td>97.5</td>
  <td>3.33</td>
  <td>0.2956</td>
  <td>0.4020</td>
  <td>0.4702</td>
  <td>0.5168</td>
  <td>0.5550</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; L2-Net</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>97.5</td>
  <td>5746.2</td>
  <td>98.2</td>
  <td>3.28</td>
  <td>0.2718</td>
  <td>0.3646</td>
  <td>0.4380</td>
  <td>0.4885</td>
  <td>0.5317</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>94.7</td>
  <td>3614.1</td>
  <td>87.8</td>
  <td>3.27</td>
  <td>0.2479</td>
  <td>0.3303</td>
  <td>0.3859</td>
  <td>0.4305</td>
  <td>0.4718</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; TFeat</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>97.0</td>
  <td>5010.7</td>
  <td>97.0</td>
  <td>3.27</td>
  <td>0.2514</td>
  <td>0.3525</td>
  <td>0.4131</td>
  <td>0.4563</td>
  <td>0.4968</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint (default)</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>96.0</td>
  <td>922.0</td>
  <td>92.8</td>
  <td>3.88</td>
  <td>0.4148</td>
  <td>0.5400</td>
  <td>0.5936</td>
  <td>0.6320</td>
  <td>0.6642</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We extract features with the default parameters and use however many are returned (about 1200 on average). Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>95.9</td>
  <td>874.8</td>
  <td>94.5</td>
  <td>3.93</td>
  <td>0.4395</td>
  <td>0.5600</td>
  <td>0.6135</td>
  <td>0.6477</td>
  <td>0.6756</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>96.3</td>
  <td>1484.5</td>
  <td>94.8</td>
  <td>3.76</td>
  <td>0.4041</td>
  <td>0.5096</td>
  <td>0.5711</td>
  <td>0.6172</td>
  <td>0.6479</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>87.0</td>
  <td>225.8</td>
  <td>63.5</td>
  <td>3.37</td>
  <td>0.2607</td>
  <td>0.3387</td>
  <td>0.3699</td>
  <td>0.3857</td>
  <td>0.3984</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>93.8</td>
  <td>481.0</td>
  <td>88.2</td>
  <td>3.78</td>
  <td>0.4028</td>
  <td>0.5143</td>
  <td>0.5669</td>
  <td>0.5948</td>
  <td>0.6117</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>96.7</td>
  <td>4626.6</td>
  <td>96.2</td>
  <td>3.46</td>
  <td>0.2205</td>
  <td>0.3698</td>
  <td>0.4620</td>
  <td>0.5172</td>
  <td>0.5649</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SURF (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>91.7</td>
  <td>2428.3</td>
  <td>84.5</td>
  <td>2.89</td>
  <td>0.0975</td>
  <td>0.1677</td>
  <td>0.2207</td>
  <td>0.2657</td>
  <td>0.3093</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  

</tbody>
</table>



<hr>
<a name="mc"></a>










<table class="display leaderboard_mvs" width="99%" cellspacing="0">
<thead>
<tr>
  <th colspan="13" style="text-align: center;" class="sorter-false">MVS &mdash; sequence &#39;milan_cathedral&#39;</th>
</tr>
<tr>
  
  <th class="all align_left" width="99%">Method</th>
  <th class="all">Date</th>
  <th class="all"><span class="balloon" data-balloon="F: Features only.&#10;M: Features and custom matches." data-balloon-break data-balloon-pos="up">Type</span></th>
  
  <th class="all"><span class="balloon" data-balloon="Ratio of images successfully&#10;registered (min: 3)." data-balloon-break data-balloon-pos="up">Ims (%)</span></th>
  <th class="all"><span class="balloon" data-balloon="Average number of 3D points&#10;in the reconstruction." data-balloon-break data-balloon-pos="up">#Pts</span></th>
  <th class="all"><span class="balloon" data-balloon="Success ratio in the&#10;3D reconstruction (%)." data-balloon-break data-balloon-pos="up">SR</span></th>
  <th class="all"><span class="balloon" data-balloon="Average Track Length&#10;(# of observations per 3D point)." data-balloon-break data-balloon-pos="up">TL</span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 5-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>5<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 10-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>10<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 15-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>15<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 20-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>20<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 25-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>25<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Average Trajectory Error." data-balloon-break data-balloon-pos="up">ATE</span></th>
  
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
  <th class="none hidden_key">Descriptor size</th>
</tr>
</thead>
<tbody>































<tr>
  
  <td class="align_left"><span class="boards_title">AKAZE (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>98.0</td>
  <td>4605.7</td>
  <td>90.2</td>
  <td>3.54</td>
  <td>0.3742</td>
  <td>0.5242</td>
  <td>0.5935</td>
  <td>0.6420</td>
  <td>0.6702</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SphereDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>98.7</td>
  <td>4561.8</td>
  <td>96.0</td>
  <td>3.04</td>
  <td>0.3799</td>
  <td>0.5325</td>
  <td>0.6131</td>
  <td>0.6570</td>
  <td>0.7016</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of AKAZE detector, and for each keypoint we extract a descriptor via CNN.</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">Brisk &#43; SSS</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-14</span></td>
  <td>F</td>
  
  <td>95.7</td>
  <td>2854.1</td>
  <td>88.2</td>
  <td>2.77</td>
  <td>0.1503</td>
  <td>0.2682</td>
  <td>0.3429</td>
  <td>0.3976</td>
  <td>0.4477</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of brisk detector with the default settings, and for each image there are at most 8K keypoints. For each keypoint, we extract a descriptor via a CNN model.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (single scale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>96.7</td>
  <td>4810.8</td>
  <td>94.5</td>
  <td>3.20</td>
  <td>0.2055</td>
  <td>0.3503</td>
  <td>0.4482</td>
  <td>0.5225</td>
  <td>0.5721</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Single-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (multiscale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>97.2</td>
  <td>6061.8</td>
  <td>96.0</td>
  <td>3.00</td>
  <td>0.1761</td>
  <td>0.3279</td>
  <td>0.4263</td>
  <td>0.5026</td>
  <td>0.5647</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Multi-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>91.1</td>
  <td>744.9</td>
  <td>83.0</td>
  <td>2.49</td>
  <td>0.0203</td>
  <td>0.0951</td>
  <td>0.1748</td>
  <td>0.2460</td>
  <td>0.3072</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>93.5</td>
  <td>1402.3</td>
  <td>87.5</td>
  <td>2.48</td>
  <td>0.0304</td>
  <td>0.1251</td>
  <td>0.2152</td>
  <td>0.2912</td>
  <td>0.3442</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>35.7</td>
  <td>86.7</td>
  <td>22.5</td>
  <td>1.79</td>
  <td>0.0003</td>
  <td>0.0028</td>
  <td>0.0075</td>
  <td>0.0123</td>
  <td>0.0165</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>81.4</td>
  <td>320.6</td>
  <td>62.5</td>
  <td>2.45</td>
  <td>0.0054</td>
  <td>0.0336</td>
  <td>0.0773</td>
  <td>0.1190</td>
  <td>0.1548</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-256D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>62.4</td>
  <td>166.5</td>
  <td>32.5</td>
  <td>2.55</td>
  <td>0.0169</td>
  <td>0.0303</td>
  <td>0.0372</td>
  <td>0.0412</td>
  <td>0.0455</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-512D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>66.8</td>
  <td>191.2</td>
  <td>41.0</td>
  <td>2.69</td>
  <td>0.0256</td>
  <td>0.0489</td>
  <td>0.0645</td>
  <td>0.0749</td>
  <td>0.0830</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-SIFT</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>84.0</td>
  <td>321.6</td>
  <td>60.5</td>
  <td>3.18</td>
  <td>0.1424</td>
  <td>0.2233</td>
  <td>0.2627</td>
  <td>0.2884</td>
  <td>0.3109</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are HOG (as in SIFT).</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ORB (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>83.7</td>
  <td>3000.3</td>
  <td>67.2</td>
  <td>2.65</td>
  <td>0.1012</td>
  <td>0.1613</td>
  <td>0.2027</td>
  <td>0.2277</td>
  <td>0.2493</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (NN matcher)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-10</span></td>
  <td>F</td>
  
  <td>92.2</td>
  <td>3106.0</td>
  <td>82.5</td>
  <td>2.95</td>
  <td>0.2665</td>
  <td>0.3724</td>
  <td>0.4313</td>
  <td>0.4724</td>
  <td>0.5107</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (custom matcher)</span><br /><span class="boards_subtitle">kp:8000, match:sift-aid<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-29</span></td>
  <td>F/M</td>
  
  <td>91.5</td>
  <td>2962.0</td>
  <td>82.0</td>
  <td>2.89</td>
  <td>0.2635</td>
  <td>0.3814</td>
  <td>0.4441</td>
  <td>0.4868</td>
  <td>0.5202</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-ContextDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>99.9</td>
  <td>7159.1</td>
  <td>99.0</td>
  <td>3.49</td>
  <td>0.4967</td>
  <td>0.6466</td>
  <td>0.7115</td>
  <td>0.7636</td>
  <td>0.7925</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">ContextDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features are quantized to uint8 and extracted from the code provided by the authors.</td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-GeoDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-08</span></td>
  <td>F</td>
  
  <td>99.7</td>
  <td>5863.3</td>
  <td>98.5</td>
  <td>3.66</td>
  <td>0.5024</td>
  <td>0.6494</td>
  <td>0.7246</td>
  <td>0.7711</td>
  <td>0.8054</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features extracted from the code provided by the authors.</td>
  <td class="hidden_value"><a href="https://arxiv.org/abs/1807.06294">https://arxiv.org/abs/1807.06294</a></td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; GeoDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>99.4</td>
  <td>5803.2</td>
  <td>98.0</td>
  <td>3.65</td>
  <td>0.4853</td>
  <td>0.6450</td>
  <td>0.7138</td>
  <td>0.7569</td>
  <td>0.7808</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; HardNet</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>99.6</td>
  <td>6992.4</td>
  <td>98.5</td>
  <td>3.53</td>
  <td>0.4737</td>
  <td>0.6270</td>
  <td>0.6945</td>
  <td>0.7446</td>
  <td>0.7752</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; L2-Net</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>99.5</td>
  <td>6584.7</td>
  <td>97.8</td>
  <td>3.51</td>
  <td>0.4818</td>
  <td>0.6342</td>
  <td>0.7093</td>
  <td>0.7595</td>
  <td>0.7870</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>97.8</td>
  <td>5042.7</td>
  <td>92.0</td>
  <td>3.47</td>
  <td>0.4324</td>
  <td>0.5683</td>
  <td>0.6433</td>
  <td>0.6835</td>
  <td>0.7085</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; TFeat</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>99.4</td>
  <td>5916.9</td>
  <td>96.5</td>
  <td>3.47</td>
  <td>0.4612</td>
  <td>0.6155</td>
  <td>0.6819</td>
  <td>0.7305</td>
  <td>0.7654</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint (default)</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>95.2</td>
  <td>1172.9</td>
  <td>85.2</td>
  <td>3.39</td>
  <td>0.3032</td>
  <td>0.4336</td>
  <td>0.5046</td>
  <td>0.5534</td>
  <td>0.5797</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We extract features with the default parameters and use however many are returned (about 1200 on average). Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>94.2</td>
  <td>841.4</td>
  <td>83.2</td>
  <td>3.32</td>
  <td>0.3051</td>
  <td>0.4243</td>
  <td>0.4946</td>
  <td>0.5351</td>
  <td>0.5616</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>95.7</td>
  <td>1524.3</td>
  <td>88.2</td>
  <td>3.44</td>
  <td>0.3035</td>
  <td>0.4387</td>
  <td>0.5072</td>
  <td>0.5523</td>
  <td>0.5886</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>33.8</td>
  <td>96.8</td>
  <td>12.8</td>
  <td>1.96</td>
  <td>0.0081</td>
  <td>0.0133</td>
  <td>0.0160</td>
  <td>0.0176</td>
  <td>0.0188</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>85.7</td>
  <td>388.5</td>
  <td>66.2</td>
  <td>3.01</td>
  <td>0.1857</td>
  <td>0.2701</td>
  <td>0.3108</td>
  <td>0.3430</td>
  <td>0.3634</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>96.5</td>
  <td>4767.1</td>
  <td>92.5</td>
  <td>3.42</td>
  <td>0.1902</td>
  <td>0.3431</td>
  <td>0.4401</td>
  <td>0.5021</td>
  <td>0.5568</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SURF (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>93.2</td>
  <td>3014.7</td>
  <td>82.5</td>
  <td>3.10</td>
  <td>0.2704</td>
  <td>0.3890</td>
  <td>0.4503</td>
  <td>0.4908</td>
  <td>0.5190</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  

</tbody>
</table>



<hr>
<a name="mr"></a>










<table class="display leaderboard_mvs" width="99%" cellspacing="0">
<thead>
<tr>
  <th colspan="13" style="text-align: center;" class="sorter-false">MVS &mdash; sequence &#39;mount_rushmore&#39;</th>
</tr>
<tr>
  
  <th class="all align_left" width="99%">Method</th>
  <th class="all">Date</th>
  <th class="all"><span class="balloon" data-balloon="F: Features only.&#10;M: Features and custom matches." data-balloon-break data-balloon-pos="up">Type</span></th>
  
  <th class="all"><span class="balloon" data-balloon="Ratio of images successfully&#10;registered (min: 3)." data-balloon-break data-balloon-pos="up">Ims (%)</span></th>
  <th class="all"><span class="balloon" data-balloon="Average number of 3D points&#10;in the reconstruction." data-balloon-break data-balloon-pos="up">#Pts</span></th>
  <th class="all"><span class="balloon" data-balloon="Success ratio in the&#10;3D reconstruction (%)." data-balloon-break data-balloon-pos="up">SR</span></th>
  <th class="all"><span class="balloon" data-balloon="Average Track Length&#10;(# of observations per 3D point)." data-balloon-break data-balloon-pos="up">TL</span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 5-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>5<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 10-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>10<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 15-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>15<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 20-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>20<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 25-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>25<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Average Trajectory Error." data-balloon-break data-balloon-pos="up">ATE</span></th>
  
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
  <th class="none hidden_key">Descriptor size</th>
</tr>
</thead>
<tbody>































<tr>
  
  <td class="align_left"><span class="boards_title">AKAZE (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>88.0</td>
  <td>2560.1</td>
  <td>84.5</td>
  <td>3.50</td>
  <td>0.1900</td>
  <td>0.2628</td>
  <td>0.3131</td>
  <td>0.3593</td>
  <td>0.3974</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SphereDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>92.7</td>
  <td>3241.5</td>
  <td>89.0</td>
  <td>3.66</td>
  <td>0.1738</td>
  <td>0.2667</td>
  <td>0.3458</td>
  <td>0.4051</td>
  <td>0.4399</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of AKAZE detector, and for each keypoint we extract a descriptor via CNN.</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">Brisk &#43; SSS</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-14</span></td>
  <td>F</td>
  
  <td>86.1</td>
  <td>2007.6</td>
  <td>77.8</td>
  <td>2.77</td>
  <td>0.0279</td>
  <td>0.0683</td>
  <td>0.1207</td>
  <td>0.1664</td>
  <td>0.2017</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of brisk detector with the default settings, and for each image there are at most 8K keypoints. For each keypoint, we extract a descriptor via a CNN model.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (single scale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>94.0</td>
  <td>4259.6</td>
  <td>90.2</td>
  <td>3.55</td>
  <td>0.1301</td>
  <td>0.2326</td>
  <td>0.3186</td>
  <td>0.3834</td>
  <td>0.4361</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Single-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (multiscale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>95.2</td>
  <td>6731.8</td>
  <td>94.0</td>
  <td>3.36</td>
  <td>0.1456</td>
  <td>0.2653</td>
  <td>0.3543</td>
  <td>0.4342</td>
  <td>0.4860</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Multi-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>81.6</td>
  <td>454.4</td>
  <td>78.8</td>
  <td>2.50</td>
  <td>0.0010</td>
  <td>0.0092</td>
  <td>0.0330</td>
  <td>0.0776</td>
  <td>0.1192</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>86.3</td>
  <td>853.8</td>
  <td>87.8</td>
  <td>2.52</td>
  <td>0.0024</td>
  <td>0.0192</td>
  <td>0.0592</td>
  <td>0.1117</td>
  <td>0.1638</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>32.2</td>
  <td>54.0</td>
  <td>27.5</td>
  <td>1.80</td>
  <td>0.0000</td>
  <td>0.0002</td>
  <td>0.0010</td>
  <td>0.0038</td>
  <td>0.0062</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>70.7</td>
  <td>191.1</td>
  <td>60.2</td>
  <td>2.42</td>
  <td>0.0003</td>
  <td>0.0040</td>
  <td>0.0140</td>
  <td>0.0316</td>
  <td>0.0504</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-256D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>59.2</td>
  <td>117.2</td>
  <td>42.2</td>
  <td>2.65</td>
  <td>0.0043</td>
  <td>0.0122</td>
  <td>0.0190</td>
  <td>0.0266</td>
  <td>0.0313</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-512D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>60.0</td>
  <td>120.6</td>
  <td>45.0</td>
  <td>2.73</td>
  <td>0.0046</td>
  <td>0.0122</td>
  <td>0.0207</td>
  <td>0.0285</td>
  <td>0.0342</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-SIFT</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>68.0</td>
  <td>164.0</td>
  <td>57.0</td>
  <td>2.87</td>
  <td>0.0293</td>
  <td>0.0585</td>
  <td>0.0799</td>
  <td>0.0972</td>
  <td>0.1116</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are HOG (as in SIFT).</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ORB (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>78.1</td>
  <td>1604.7</td>
  <td>69.8</td>
  <td>2.96</td>
  <td>0.0640</td>
  <td>0.1070</td>
  <td>0.1402</td>
  <td>0.1691</td>
  <td>0.1905</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (NN matcher)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-10</span></td>
  <td>F</td>
  
  <td>78.3</td>
  <td>1244.1</td>
  <td>66.0</td>
  <td>2.86</td>
  <td>0.0693</td>
  <td>0.1117</td>
  <td>0.1428</td>
  <td>0.1707</td>
  <td>0.1927</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (custom matcher)</span><br /><span class="boards_subtitle">kp:8000, match:sift-aid<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-29</span></td>
  <td>F/M</td>
  
  <td>75.7</td>
  <td>1066.4</td>
  <td>65.0</td>
  <td>2.79</td>
  <td>0.0656</td>
  <td>0.1071</td>
  <td>0.1367</td>
  <td>0.1638</td>
  <td>0.1835</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-ContextDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>94.9</td>
  <td>4393.5</td>
  <td>95.0</td>
  <td>3.66</td>
  <td>0.2958</td>
  <td>0.4132</td>
  <td>0.4788</td>
  <td>0.5322</td>
  <td>0.5700</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">ContextDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features are quantized to uint8 and extracted from the code provided by the authors.</td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-GeoDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-08</span></td>
  <td>F</td>
  
  <td>93.9</td>
  <td>4283.5</td>
  <td>91.5</td>
  <td>3.64</td>
  <td>0.3019</td>
  <td>0.4028</td>
  <td>0.4662</td>
  <td>0.5188</td>
  <td>0.5631</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features extracted from the code provided by the authors.</td>
  <td class="hidden_value"><a href="https://arxiv.org/abs/1807.06294">https://arxiv.org/abs/1807.06294</a></td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; GeoDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>94.2</td>
  <td>4258.4</td>
  <td>91.2</td>
  <td>3.66</td>
  <td>0.3333</td>
  <td>0.4335</td>
  <td>0.4907</td>
  <td>0.5437</td>
  <td>0.5759</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; HardNet</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>94.8</td>
  <td>5031.3</td>
  <td>91.8</td>
  <td>3.61</td>
  <td>0.3225</td>
  <td>0.4429</td>
  <td>0.5106</td>
  <td>0.5660</td>
  <td>0.6041</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; L2-Net</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>92.9</td>
  <td>4193.4</td>
  <td>89.5</td>
  <td>3.57</td>
  <td>0.2987</td>
  <td>0.3976</td>
  <td>0.4667</td>
  <td>0.5171</td>
  <td>0.5530</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>85.6</td>
  <td>2470.7</td>
  <td>83.0</td>
  <td>3.29</td>
  <td>0.1986</td>
  <td>0.2710</td>
  <td>0.3181</td>
  <td>0.3563</td>
  <td>0.3825</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; TFeat</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>91.1</td>
  <td>3276.1</td>
  <td>86.8</td>
  <td>3.50</td>
  <td>0.2535</td>
  <td>0.3399</td>
  <td>0.4022</td>
  <td>0.4485</td>
  <td>0.4811</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint (default)</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>89.9</td>
  <td>494.1</td>
  <td>87.8</td>
  <td>4.14</td>
  <td>0.1925</td>
  <td>0.2929</td>
  <td>0.3569</td>
  <td>0.4057</td>
  <td>0.4463</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We extract features with the default parameters and use however many are returned (about 1200 on average). Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>91.2</td>
  <td>580.9</td>
  <td>87.0</td>
  <td>4.24</td>
  <td>0.2113</td>
  <td>0.3033</td>
  <td>0.3670</td>
  <td>0.4165</td>
  <td>0.4602</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>91.5</td>
  <td>1048.4</td>
  <td>89.8</td>
  <td>4.08</td>
  <td>0.2214</td>
  <td>0.3216</td>
  <td>0.3859</td>
  <td>0.4300</td>
  <td>0.4717</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>75.2</td>
  <td>113.6</td>
  <td>64.2</td>
  <td>3.42</td>
  <td>0.0501</td>
  <td>0.0837</td>
  <td>0.1125</td>
  <td>0.1370</td>
  <td>0.1558</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>83.0</td>
  <td>246.6</td>
  <td>83.2</td>
  <td>3.84</td>
  <td>0.1380</td>
  <td>0.2119</td>
  <td>0.2704</td>
  <td>0.3132</td>
  <td>0.3438</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>94.4</td>
  <td>3764.8</td>
  <td>92.0</td>
  <td>3.85</td>
  <td>0.1489</td>
  <td>0.2427</td>
  <td>0.3203</td>
  <td>0.3796</td>
  <td>0.4292</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SURF (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>83.3</td>
  <td>1508.4</td>
  <td>76.5</td>
  <td>3.23</td>
  <td>0.1146</td>
  <td>0.1709</td>
  <td>0.2126</td>
  <td>0.2475</td>
  <td>0.2775</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  

</tbody>
</table>



<hr>
<a name="psm"></a>










<table class="display leaderboard_mvs" width="99%" cellspacing="0">
<thead>
<tr>
  <th colspan="13" style="text-align: center;" class="sorter-false">MVS &mdash; sequence &#39;piazza_san_marco&#39;</th>
</tr>
<tr>
  
  <th class="all align_left" width="99%">Method</th>
  <th class="all">Date</th>
  <th class="all"><span class="balloon" data-balloon="F: Features only.&#10;M: Features and custom matches." data-balloon-break data-balloon-pos="up">Type</span></th>
  
  <th class="all"><span class="balloon" data-balloon="Ratio of images successfully&#10;registered (min: 3)." data-balloon-break data-balloon-pos="up">Ims (%)</span></th>
  <th class="all"><span class="balloon" data-balloon="Average number of 3D points&#10;in the reconstruction." data-balloon-break data-balloon-pos="up">#Pts</span></th>
  <th class="all"><span class="balloon" data-balloon="Success ratio in the&#10;3D reconstruction (%)." data-balloon-break data-balloon-pos="up">SR</span></th>
  <th class="all"><span class="balloon" data-balloon="Average Track Length&#10;(# of observations per 3D point)." data-balloon-break data-balloon-pos="up">TL</span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 5-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>5<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 10-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>10<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 15-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>15<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 20-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>20<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 25-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>25<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Average Trajectory Error." data-balloon-break data-balloon-pos="up">ATE</span></th>
  
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
  <th class="none hidden_key">Descriptor size</th>
</tr>
</thead>
<tbody>































<tr>
  
  <td class="align_left"><span class="boards_title">AKAZE (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>90.2</td>
  <td>5212.9</td>
  <td>84.8</td>
  <td>2.37</td>
  <td>0.1090</td>
  <td>0.1960</td>
  <td>0.2543</td>
  <td>0.3059</td>
  <td>0.3479</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SphereDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>96.6</td>
  <td>5929.9</td>
  <td>96.0</td>
  <td>2.47</td>
  <td>0.1927</td>
  <td>0.3246</td>
  <td>0.4122</td>
  <td>0.4794</td>
  <td>0.5291</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of AKAZE detector, and for each keypoint we extract a descriptor via CNN.</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">Brisk &#43; SSS</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-14</span></td>
  <td>F</td>
  
  <td>92.3</td>
  <td>4004.4</td>
  <td>89.5</td>
  <td>2.29</td>
  <td>0.0765</td>
  <td>0.1823</td>
  <td>0.2678</td>
  <td>0.3353</td>
  <td>0.3876</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of brisk detector with the default settings, and for each image there are at most 8K keypoints. For each keypoint, we extract a descriptor via a CNN model.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (single scale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>95.9</td>
  <td>5493.2</td>
  <td>93.5</td>
  <td>2.41</td>
  <td>0.1385</td>
  <td>0.2715</td>
  <td>0.3673</td>
  <td>0.4293</td>
  <td>0.4868</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Single-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (multiscale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>96.8</td>
  <td>6287.8</td>
  <td>96.5</td>
  <td>2.42</td>
  <td>0.1290</td>
  <td>0.2761</td>
  <td>0.3746</td>
  <td>0.4409</td>
  <td>0.4992</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Multi-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>65.6</td>
  <td>526.3</td>
  <td>56.5</td>
  <td>2.09</td>
  <td>0.0058</td>
  <td>0.0262</td>
  <td>0.0476</td>
  <td>0.0674</td>
  <td>0.0848</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>81.8</td>
  <td>1718.9</td>
  <td>78.0</td>
  <td>2.08</td>
  <td>0.0178</td>
  <td>0.0764</td>
  <td>0.1378</td>
  <td>0.1946</td>
  <td>0.2381</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>%!f(int64=0)</td>
  <td>0.0</td>
  <td>0.0</td>
  <td>0.00</td>
  <td>0.0000</td>
  <td>0.0000</td>
  <td>0.0000</td>
  <td>0.0000</td>
  <td>0.0000</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>51.8</td>
  <td>112.2</td>
  <td>22.2</td>
  <td>2.13</td>
  <td>0.0001</td>
  <td>0.0006</td>
  <td>0.0010</td>
  <td>0.0026</td>
  <td>0.0034</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-256D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>11.6</td>
  <td>63.5</td>
  <td>21.8</td>
  <td>1.08</td>
  <td>0.0001</td>
  <td>0.0005</td>
  <td>0.0010</td>
  <td>0.0014</td>
  <td>0.0017</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-512D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>27.6</td>
  <td>95.0</td>
  <td>29.8</td>
  <td>1.63</td>
  <td>0.0003</td>
  <td>0.0011</td>
  <td>0.0024</td>
  <td>0.0040</td>
  <td>0.0048</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-SIFT</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>55.6</td>
  <td>141.1</td>
  <td>34.2</td>
  <td>2.27</td>
  <td>0.0036</td>
  <td>0.0089</td>
  <td>0.0148</td>
  <td>0.0176</td>
  <td>0.0215</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are HOG (as in SIFT).</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ORB (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>87.5</td>
  <td>4718.8</td>
  <td>86.8</td>
  <td>2.18</td>
  <td>0.0192</td>
  <td>0.0666</td>
  <td>0.1302</td>
  <td>0.1867</td>
  <td>0.2426</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (NN matcher)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-10</span></td>
  <td>F</td>
  
  <td>83.0</td>
  <td>2717.4</td>
  <td>77.2</td>
  <td>2.24</td>
  <td>0.0687</td>
  <td>0.1341</td>
  <td>0.1834</td>
  <td>0.2265</td>
  <td>0.2614</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (custom matcher)</span><br /><span class="boards_subtitle">kp:8000, match:sift-aid<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-29</span></td>
  <td>F/M</td>
  
  <td>82.4</td>
  <td>2633.9</td>
  <td>74.2</td>
  <td>2.24</td>
  <td>0.0704</td>
  <td>0.1389</td>
  <td>0.1903</td>
  <td>0.2296</td>
  <td>0.2672</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-ContextDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>98.9</td>
  <td>8508.5</td>
  <td>98.0</td>
  <td>2.65</td>
  <td>0.2755</td>
  <td>0.4066</td>
  <td>0.4852</td>
  <td>0.5414</td>
  <td>0.5906</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">ContextDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features are quantized to uint8 and extracted from the code provided by the authors.</td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-GeoDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-08</span></td>
  <td>F</td>
  
  <td>98.1</td>
  <td>7387.0</td>
  <td>95.0</td>
  <td>2.70</td>
  <td>0.2740</td>
  <td>0.4013</td>
  <td>0.4772</td>
  <td>0.5292</td>
  <td>0.5787</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features extracted from the code provided by the authors.</td>
  <td class="hidden_value"><a href="https://arxiv.org/abs/1807.06294">https://arxiv.org/abs/1807.06294</a></td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; GeoDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>97.5</td>
  <td>7295.3</td>
  <td>94.8</td>
  <td>2.56</td>
  <td>0.2476</td>
  <td>0.3512</td>
  <td>0.4137</td>
  <td>0.4679</td>
  <td>0.5185</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; HardNet</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>98.4</td>
  <td>8333.2</td>
  <td>99.2</td>
  <td>2.57</td>
  <td>0.2565</td>
  <td>0.3783</td>
  <td>0.4612</td>
  <td>0.5187</td>
  <td>0.5603</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; L2-Net</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>97.3</td>
  <td>7765.5</td>
  <td>97.0</td>
  <td>2.47</td>
  <td>0.2093</td>
  <td>0.3237</td>
  <td>0.4000</td>
  <td>0.4572</td>
  <td>0.5014</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>91.2</td>
  <td>5351.7</td>
  <td>87.2</td>
  <td>2.35</td>
  <td>0.1394</td>
  <td>0.2188</td>
  <td>0.2797</td>
  <td>0.3339</td>
  <td>0.3677</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; TFeat</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>95.8</td>
  <td>6975.9</td>
  <td>94.8</td>
  <td>2.41</td>
  <td>0.1664</td>
  <td>0.2763</td>
  <td>0.3557</td>
  <td>0.4210</td>
  <td>0.4733</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint (default)</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>87.3</td>
  <td>1317.6</td>
  <td>80.2</td>
  <td>2.46</td>
  <td>0.1420</td>
  <td>0.2472</td>
  <td>0.3163</td>
  <td>0.3597</td>
  <td>0.3947</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We extract features with the default parameters and use however many are returned (about 1200 on average). Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>83.0</td>
  <td>857.1</td>
  <td>71.5</td>
  <td>2.45</td>
  <td>0.1234</td>
  <td>0.2109</td>
  <td>0.2659</td>
  <td>0.3044</td>
  <td>0.3314</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>89.2</td>
  <td>1695.3</td>
  <td>82.5</td>
  <td>2.47</td>
  <td>0.1420</td>
  <td>0.2534</td>
  <td>0.3269</td>
  <td>0.3727</td>
  <td>0.4123</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>3.2</td>
  <td>26.7</td>
  <td>4.0</td>
  <td>0.58</td>
  <td>0.0000</td>
  <td>0.0001</td>
  <td>0.0002</td>
  <td>0.0002</td>
  <td>0.0003</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>41.1</td>
  <td>246.5</td>
  <td>45.5</td>
  <td>1.79</td>
  <td>0.0461</td>
  <td>0.0804</td>
  <td>0.0990</td>
  <td>0.1101</td>
  <td>0.1176</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>95.8</td>
  <td>6173.1</td>
  <td>93.8</td>
  <td>2.45</td>
  <td>0.1470</td>
  <td>0.2804</td>
  <td>0.3695</td>
  <td>0.4303</td>
  <td>0.4829</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SURF (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>83.4</td>
  <td>3696.9</td>
  <td>74.5</td>
  <td>2.25</td>
  <td>0.0784</td>
  <td>0.1534</td>
  <td>0.2072</td>
  <td>0.2466</td>
  <td>0.2803</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  

</tbody>
</table>



<hr>
<a name="rs"></a>










<table class="display leaderboard_mvs" width="99%" cellspacing="0">
<thead>
<tr>
  <th colspan="13" style="text-align: center;" class="sorter-false">MVS &mdash; sequence &#39;reichstag&#39;</th>
</tr>
<tr>
  
  <th class="all align_left" width="99%">Method</th>
  <th class="all">Date</th>
  <th class="all"><span class="balloon" data-balloon="F: Features only.&#10;M: Features and custom matches." data-balloon-break data-balloon-pos="up">Type</span></th>
  
  <th class="all"><span class="balloon" data-balloon="Ratio of images successfully&#10;registered (min: 3)." data-balloon-break data-balloon-pos="up">Ims (%)</span></th>
  <th class="all"><span class="balloon" data-balloon="Average number of 3D points&#10;in the reconstruction." data-balloon-break data-balloon-pos="up">#Pts</span></th>
  <th class="all"><span class="balloon" data-balloon="Success ratio in the&#10;3D reconstruction (%)." data-balloon-break data-balloon-pos="up">SR</span></th>
  <th class="all"><span class="balloon" data-balloon="Average Track Length&#10;(# of observations per 3D point)." data-balloon-break data-balloon-pos="up">TL</span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 5-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>5<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 10-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>10<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 15-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>15<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 20-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>20<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 25-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>25<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Average Trajectory Error." data-balloon-break data-balloon-pos="up">ATE</span></th>
  
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
  <th class="none hidden_key">Descriptor size</th>
</tr>
</thead>
<tbody>































<tr>
  
  <td class="align_left"><span class="boards_title">AKAZE (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>95.7</td>
  <td>4128.4</td>
  <td>90.5</td>
  <td>3.48</td>
  <td>0.3067</td>
  <td>0.4024</td>
  <td>0.4708</td>
  <td>0.5249</td>
  <td>0.5651</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SphereDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>96.4</td>
  <td>2565.4</td>
  <td>97.0</td>
  <td>2.93</td>
  <td>0.2516</td>
  <td>0.3680</td>
  <td>0.4310</td>
  <td>0.4832</td>
  <td>0.5263</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of AKAZE detector, and for each keypoint we extract a descriptor via CNN.</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">Brisk &#43; SSS</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-14</span></td>
  <td>F</td>
  
  <td>94.7</td>
  <td>1743.9</td>
  <td>93.0</td>
  <td>2.79</td>
  <td>0.1505</td>
  <td>0.2669</td>
  <td>0.3511</td>
  <td>0.4131</td>
  <td>0.4629</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of brisk detector with the default settings, and for each image there are at most 8K keypoints. For each keypoint, we extract a descriptor via a CNN model.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (single scale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>94.4</td>
  <td>3590.9</td>
  <td>97.5</td>
  <td>3.20</td>
  <td>0.1229</td>
  <td>0.2517</td>
  <td>0.3340</td>
  <td>0.3847</td>
  <td>0.4260</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Single-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (multiscale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>95.6</td>
  <td>4299.7</td>
  <td>97.0</td>
  <td>3.04</td>
  <td>0.1147</td>
  <td>0.2860</td>
  <td>0.3690</td>
  <td>0.4264</td>
  <td>0.4724</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Multi-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>86.8</td>
  <td>863.4</td>
  <td>91.0</td>
  <td>2.36</td>
  <td>0.0117</td>
  <td>0.0724</td>
  <td>0.1275</td>
  <td>0.1759</td>
  <td>0.2156</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>90.8</td>
  <td>1653.1</td>
  <td>94.0</td>
  <td>2.36</td>
  <td>0.0143</td>
  <td>0.0818</td>
  <td>0.1566</td>
  <td>0.2076</td>
  <td>0.2548</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>62.6</td>
  <td>121.4</td>
  <td>45.8</td>
  <td>2.30</td>
  <td>0.0016</td>
  <td>0.0113</td>
  <td>0.0223</td>
  <td>0.0319</td>
  <td>0.0417</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>78.7</td>
  <td>369.2</td>
  <td>77.8</td>
  <td>2.36</td>
  <td>0.0089</td>
  <td>0.0459</td>
  <td>0.0895</td>
  <td>0.1200</td>
  <td>0.1482</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-256D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>73.2</td>
  <td>162.5</td>
  <td>53.0</td>
  <td>2.91</td>
  <td>0.0547</td>
  <td>0.0973</td>
  <td>0.1209</td>
  <td>0.1389</td>
  <td>0.1532</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-512D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>75.6</td>
  <td>171.2</td>
  <td>61.7</td>
  <td>3.01</td>
  <td>0.0498</td>
  <td>0.1074</td>
  <td>0.1424</td>
  <td>0.1635</td>
  <td>0.1826</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-SIFT</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>82.3</td>
  <td>341.6</td>
  <td>79.0</td>
  <td>3.14</td>
  <td>0.0984</td>
  <td>0.1749</td>
  <td>0.2254</td>
  <td>0.2604</td>
  <td>0.2864</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are HOG (as in SIFT).</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ORB (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>88.6</td>
  <td>2551.1</td>
  <td>83.0</td>
  <td>2.76</td>
  <td>0.1663</td>
  <td>0.2417</td>
  <td>0.3006</td>
  <td>0.3463</td>
  <td>0.3810</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (NN matcher)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-10</span></td>
  <td>F</td>
  
  <td>93.3</td>
  <td>2182.3</td>
  <td>92.0</td>
  <td>2.83</td>
  <td>0.1669</td>
  <td>0.2666</td>
  <td>0.3325</td>
  <td>0.3850</td>
  <td>0.4194</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (custom matcher)</span><br /><span class="boards_subtitle">kp:8000, match:sift-aid<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-29</span></td>
  <td>F/M</td>
  
  <td>92.4</td>
  <td>2068.1</td>
  <td>91.2</td>
  <td>2.80</td>
  <td>0.1584</td>
  <td>0.2514</td>
  <td>0.3177</td>
  <td>0.3605</td>
  <td>0.4051</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-ContextDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>98.0</td>
  <td>6330.9</td>
  <td>98.5</td>
  <td>3.28</td>
  <td>0.2695</td>
  <td>0.3867</td>
  <td>0.4540</td>
  <td>0.4986</td>
  <td>0.5341</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">ContextDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features are quantized to uint8 and extracted from the code provided by the authors.</td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-GeoDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-08</span></td>
  <td>F</td>
  
  <td>97.4</td>
  <td>5624.5</td>
  <td>95.0</td>
  <td>3.38</td>
  <td>0.3033</td>
  <td>0.4097</td>
  <td>0.4788</td>
  <td>0.5296</td>
  <td>0.5633</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features extracted from the code provided by the authors.</td>
  <td class="hidden_value"><a href="https://arxiv.org/abs/1807.06294">https://arxiv.org/abs/1807.06294</a></td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; GeoDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>97.3</td>
  <td>5817.2</td>
  <td>97.0</td>
  <td>3.41</td>
  <td>0.3031</td>
  <td>0.3966</td>
  <td>0.4632</td>
  <td>0.5150</td>
  <td>0.5497</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; HardNet</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>97.8</td>
  <td>6797.9</td>
  <td>98.2</td>
  <td>3.33</td>
  <td>0.2767</td>
  <td>0.3813</td>
  <td>0.4477</td>
  <td>0.4950</td>
  <td>0.5404</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; L2-Net</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>97.6</td>
  <td>6504.8</td>
  <td>98.8</td>
  <td>3.25</td>
  <td>0.2300</td>
  <td>0.3503</td>
  <td>0.4192</td>
  <td>0.4664</td>
  <td>0.5079</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>96.2</td>
  <td>4660.9</td>
  <td>92.2</td>
  <td>3.28</td>
  <td>0.2605</td>
  <td>0.3551</td>
  <td>0.4216</td>
  <td>0.4699</td>
  <td>0.5110</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; TFeat</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>96.8</td>
  <td>5811.1</td>
  <td>97.0</td>
  <td>3.19</td>
  <td>0.2201</td>
  <td>0.3241</td>
  <td>0.3929</td>
  <td>0.4449</td>
  <td>0.4812</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint (default)</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>92.9</td>
  <td>1088.8</td>
  <td>93.0</td>
  <td>3.54</td>
  <td>0.2075</td>
  <td>0.3153</td>
  <td>0.3722</td>
  <td>0.4203</td>
  <td>0.4525</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We extract features with the default parameters and use however many are returned (about 1200 on average). Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>91.7</td>
  <td>914.4</td>
  <td>92.0</td>
  <td>3.62</td>
  <td>0.1855</td>
  <td>0.2772</td>
  <td>0.3370</td>
  <td>0.3825</td>
  <td>0.4207</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>93.1</td>
  <td>1585.5</td>
  <td>94.5</td>
  <td>3.50</td>
  <td>0.1955</td>
  <td>0.2930</td>
  <td>0.3598</td>
  <td>0.4067</td>
  <td>0.4375</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>76.8</td>
  <td>189.6</td>
  <td>66.5</td>
  <td>3.05</td>
  <td>0.0376</td>
  <td>0.0915</td>
  <td>0.1210</td>
  <td>0.1440</td>
  <td>0.1620</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>87.6</td>
  <td>463.8</td>
  <td>88.5</td>
  <td>3.39</td>
  <td>0.1346</td>
  <td>0.2248</td>
  <td>0.2769</td>
  <td>0.3134</td>
  <td>0.3449</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>95.1</td>
  <td>4993.1</td>
  <td>95.8</td>
  <td>3.43</td>
  <td>0.1759</td>
  <td>0.2915</td>
  <td>0.3651</td>
  <td>0.4184</td>
  <td>0.4571</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SURF (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>91.3</td>
  <td>2316.1</td>
  <td>87.2</td>
  <td>3.08</td>
  <td>0.1986</td>
  <td>0.2845</td>
  <td>0.3438</td>
  <td>0.3844</td>
  <td>0.4171</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  

</tbody>
</table>



<hr>
<a name="sf"></a>










<table class="display leaderboard_mvs" width="99%" cellspacing="0">
<thead>
<tr>
  <th colspan="13" style="text-align: center;" class="sorter-false">MVS &mdash; sequence &#39;sagrada_familia&#39;</th>
</tr>
<tr>
  
  <th class="all align_left" width="99%">Method</th>
  <th class="all">Date</th>
  <th class="all"><span class="balloon" data-balloon="F: Features only.&#10;M: Features and custom matches." data-balloon-break data-balloon-pos="up">Type</span></th>
  
  <th class="all"><span class="balloon" data-balloon="Ratio of images successfully&#10;registered (min: 3)." data-balloon-break data-balloon-pos="up">Ims (%)</span></th>
  <th class="all"><span class="balloon" data-balloon="Average number of 3D points&#10;in the reconstruction." data-balloon-break data-balloon-pos="up">#Pts</span></th>
  <th class="all"><span class="balloon" data-balloon="Success ratio in the&#10;3D reconstruction (%)." data-balloon-break data-balloon-pos="up">SR</span></th>
  <th class="all"><span class="balloon" data-balloon="Average Track Length&#10;(# of observations per 3D point)." data-balloon-break data-balloon-pos="up">TL</span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 5-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>5<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 10-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>10<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 15-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>15<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 20-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>20<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 25-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>25<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Average Trajectory Error." data-balloon-break data-balloon-pos="up">ATE</span></th>
  
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
  <th class="none hidden_key">Descriptor size</th>
</tr>
</thead>
<tbody>































<tr>
  
  <td class="align_left"><span class="boards_title">AKAZE (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>94.8</td>
  <td>5940.4</td>
  <td>86.0</td>
  <td>3.34</td>
  <td>0.4935</td>
  <td>0.5487</td>
  <td>0.5709</td>
  <td>0.5878</td>
  <td>0.6045</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SphereDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>98.3</td>
  <td>6872.1</td>
  <td>94.0</td>
  <td>3.48</td>
  <td>0.6073</td>
  <td>0.7024</td>
  <td>0.7381</td>
  <td>0.7566</td>
  <td>0.7772</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of AKAZE detector, and for each keypoint we extract a descriptor via CNN.</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">Brisk &#43; SSS</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-14</span></td>
  <td>F</td>
  
  <td>96.7</td>
  <td>4768.0</td>
  <td>91.2</td>
  <td>2.86</td>
  <td>0.3090</td>
  <td>0.3994</td>
  <td>0.4466</td>
  <td>0.4838</td>
  <td>0.5194</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of brisk detector with the default settings, and for each image there are at most 8K keypoints. For each keypoint, we extract a descriptor via a CNN model.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (single scale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>89.5</td>
  <td>7264.4</td>
  <td>87.5</td>
  <td>3.15</td>
  <td>0.3794</td>
  <td>0.4680</td>
  <td>0.5189</td>
  <td>0.5427</td>
  <td>0.5621</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Single-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (multiscale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>89.9</td>
  <td>10021.2</td>
  <td>90.5</td>
  <td>2.98</td>
  <td>0.3677</td>
  <td>0.4655</td>
  <td>0.5083</td>
  <td>0.5418</td>
  <td>0.5670</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Multi-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>73.3</td>
  <td>380.0</td>
  <td>50.5</td>
  <td>2.30</td>
  <td>0.0129</td>
  <td>0.0379</td>
  <td>0.0621</td>
  <td>0.0849</td>
  <td>0.1032</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>79.6</td>
  <td>938.9</td>
  <td>66.2</td>
  <td>2.32</td>
  <td>0.0322</td>
  <td>0.0897</td>
  <td>0.1365</td>
  <td>0.1714</td>
  <td>0.2008</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>3.0</td>
  <td>17.2</td>
  <td>0.3</td>
  <td>0.60</td>
  <td>0.0000</td>
  <td>0.0000</td>
  <td>0.0000</td>
  <td>0.0000</td>
  <td>0.0000</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>29.8</td>
  <td>103.7</td>
  <td>18.2</td>
  <td>1.65</td>
  <td>0.0001</td>
  <td>0.0009</td>
  <td>0.0018</td>
  <td>0.0031</td>
  <td>0.0044</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-256D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>67.9</td>
  <td>205.1</td>
  <td>42.8</td>
  <td>2.82</td>
  <td>0.0704</td>
  <td>0.0888</td>
  <td>0.0985</td>
  <td>0.1048</td>
  <td>0.1102</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-512D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>67.4</td>
  <td>194.6</td>
  <td>44.8</td>
  <td>2.73</td>
  <td>0.0593</td>
  <td>0.0786</td>
  <td>0.0889</td>
  <td>0.0958</td>
  <td>0.1012</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-SIFT</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>76.8</td>
  <td>407.4</td>
  <td>62.7</td>
  <td>2.95</td>
  <td>0.1847</td>
  <td>0.2118</td>
  <td>0.2236</td>
  <td>0.2345</td>
  <td>0.2443</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are HOG (as in SIFT).</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ORB (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>90.3</td>
  <td>4267.3</td>
  <td>77.8</td>
  <td>2.90</td>
  <td>0.2972</td>
  <td>0.3519</td>
  <td>0.3802</td>
  <td>0.3998</td>
  <td>0.4188</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (NN matcher)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-10</span></td>
  <td>F</td>
  
  <td>91.3</td>
  <td>5160.2</td>
  <td>77.5</td>
  <td>2.96</td>
  <td>0.3819</td>
  <td>0.4405</td>
  <td>0.4663</td>
  <td>0.4851</td>
  <td>0.5012</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (custom matcher)</span><br /><span class="boards_subtitle">kp:8000, match:sift-aid<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-29</span></td>
  <td>F/M</td>
  
  <td>90.3</td>
  <td>4920.5</td>
  <td>74.8</td>
  <td>2.90</td>
  <td>0.3649</td>
  <td>0.4322</td>
  <td>0.4562</td>
  <td>0.4720</td>
  <td>0.4852</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-ContextDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>99.2</td>
  <td>7398.7</td>
  <td>97.8</td>
  <td>3.82</td>
  <td>0.6873</td>
  <td>0.7552</td>
  <td>0.7826</td>
  <td>0.7969</td>
  <td>0.8147</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">ContextDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features are quantized to uint8 and extracted from the code provided by the authors.</td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-GeoDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-08</span></td>
  <td>F</td>
  
  <td>98.7</td>
  <td>6953.4</td>
  <td>96.0</td>
  <td>3.80</td>
  <td>0.6939</td>
  <td>0.7666</td>
  <td>0.7983</td>
  <td>0.8096</td>
  <td>0.8279</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features extracted from the code provided by the authors.</td>
  <td class="hidden_value"><a href="https://arxiv.org/abs/1807.06294">https://arxiv.org/abs/1807.06294</a></td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; GeoDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>98.7</td>
  <td>7011.0</td>
  <td>95.5</td>
  <td>3.81</td>
  <td>0.6903</td>
  <td>0.7544</td>
  <td>0.7740</td>
  <td>0.7939</td>
  <td>0.8091</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; HardNet</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>99.4</td>
  <td>7983.0</td>
  <td>98.5</td>
  <td>3.76</td>
  <td>0.7048</td>
  <td>0.7742</td>
  <td>0.8030</td>
  <td>0.8179</td>
  <td>0.8344</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; L2-Net</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>98.8</td>
  <td>7596.6</td>
  <td>96.2</td>
  <td>3.67</td>
  <td>0.6693</td>
  <td>0.7417</td>
  <td>0.7682</td>
  <td>0.7852</td>
  <td>0.8004</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>95.5</td>
  <td>6397.0</td>
  <td>85.0</td>
  <td>3.40</td>
  <td>0.5238</td>
  <td>0.5770</td>
  <td>0.6002</td>
  <td>0.6150</td>
  <td>0.6292</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; TFeat</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>97.5</td>
  <td>7119.6</td>
  <td>92.2</td>
  <td>3.55</td>
  <td>0.5976</td>
  <td>0.6585</td>
  <td>0.6857</td>
  <td>0.7047</td>
  <td>0.7228</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint (default)</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>88.0</td>
  <td>1373.6</td>
  <td>81.0</td>
  <td>3.54</td>
  <td>0.4117</td>
  <td>0.4641</td>
  <td>0.4853</td>
  <td>0.4977</td>
  <td>0.5074</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We extract features with the default parameters and use however many are returned (about 1200 on average). Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>87.2</td>
  <td>981.2</td>
  <td>79.2</td>
  <td>3.54</td>
  <td>0.3896</td>
  <td>0.4424</td>
  <td>0.4623</td>
  <td>0.4737</td>
  <td>0.4858</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>88.5</td>
  <td>1838.3</td>
  <td>82.2</td>
  <td>3.56</td>
  <td>0.4220</td>
  <td>0.4732</td>
  <td>0.4956</td>
  <td>0.5074</td>
  <td>0.5159</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>76.2</td>
  <td>231.9</td>
  <td>55.8</td>
  <td>3.10</td>
  <td>0.1653</td>
  <td>0.1907</td>
  <td>0.2019</td>
  <td>0.2115</td>
  <td>0.2179</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>83.8</td>
  <td>472.3</td>
  <td>71.8</td>
  <td>3.38</td>
  <td>0.3075</td>
  <td>0.3490</td>
  <td>0.3634</td>
  <td>0.3772</td>
  <td>0.3851</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>89.5</td>
  <td>6354.4</td>
  <td>87.5</td>
  <td>3.34</td>
  <td>0.3630</td>
  <td>0.4459</td>
  <td>0.4828</td>
  <td>0.5083</td>
  <td>0.5269</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SURF (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>91.6</td>
  <td>4425.8</td>
  <td>79.0</td>
  <td>3.08</td>
  <td>0.4146</td>
  <td>0.4687</td>
  <td>0.4977</td>
  <td>0.5097</td>
  <td>0.5208</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  

</tbody>
</table>



<hr>
<a name="spc"></a>










<table class="display leaderboard_mvs" width="99%" cellspacing="0">
<thead>
<tr>
  <th colspan="13" style="text-align: center;" class="sorter-false">MVS &mdash; sequence &#39;st_pauls_cathedral&#39;</th>
</tr>
<tr>
  
  <th class="all align_left" width="99%">Method</th>
  <th class="all">Date</th>
  <th class="all"><span class="balloon" data-balloon="F: Features only.&#10;M: Features and custom matches." data-balloon-break data-balloon-pos="up">Type</span></th>
  
  <th class="all"><span class="balloon" data-balloon="Ratio of images successfully&#10;registered (min: 3)." data-balloon-break data-balloon-pos="up">Ims (%)</span></th>
  <th class="all"><span class="balloon" data-balloon="Average number of 3D points&#10;in the reconstruction." data-balloon-break data-balloon-pos="up">#Pts</span></th>
  <th class="all"><span class="balloon" data-balloon="Success ratio in the&#10;3D reconstruction (%)." data-balloon-break data-balloon-pos="up">SR</span></th>
  <th class="all"><span class="balloon" data-balloon="Average Track Length&#10;(# of observations per 3D point)." data-balloon-break data-balloon-pos="up">TL</span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 5-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>5<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 10-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>10<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 15-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>15<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 20-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>20<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 25-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>25<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Average Trajectory Error." data-balloon-break data-balloon-pos="up">ATE</span></th>
  
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
  <th class="none hidden_key">Descriptor size</th>
</tr>
</thead>
<tbody>































<tr>
  
  <td class="align_left"><span class="boards_title">AKAZE (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>98.1</td>
  <td>5029.6</td>
  <td>93.0</td>
  <td>3.34</td>
  <td>0.4401</td>
  <td>0.5381</td>
  <td>0.6054</td>
  <td>0.6487</td>
  <td>0.6816</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SphereDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>99.4</td>
  <td>3838.2</td>
  <td>98.2</td>
  <td>3.34</td>
  <td>0.4444</td>
  <td>0.5737</td>
  <td>0.6481</td>
  <td>0.6951</td>
  <td>0.7375</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of AKAZE detector, and for each keypoint we extract a descriptor via CNN.</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">Brisk &#43; SSS</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-14</span></td>
  <td>F</td>
  
  <td>97.9</td>
  <td>2951.3</td>
  <td>94.0</td>
  <td>2.82</td>
  <td>0.2455</td>
  <td>0.3941</td>
  <td>0.4849</td>
  <td>0.5456</td>
  <td>0.6024</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of brisk detector with the default settings, and for each image there are at most 8K keypoints. For each keypoint, we extract a descriptor via a CNN model.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (single scale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>94.5</td>
  <td>3953.4</td>
  <td>92.5</td>
  <td>3.12</td>
  <td>0.2786</td>
  <td>0.4211</td>
  <td>0.5070</td>
  <td>0.5661</td>
  <td>0.6065</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Single-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (multiscale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>95.3</td>
  <td>5052.5</td>
  <td>94.0</td>
  <td>2.92</td>
  <td>0.2677</td>
  <td>0.4203</td>
  <td>0.5044</td>
  <td>0.5542</td>
  <td>0.6053</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Multi-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>86.6</td>
  <td>764.3</td>
  <td>72.5</td>
  <td>2.36</td>
  <td>0.0304</td>
  <td>0.1220</td>
  <td>0.2125</td>
  <td>0.2769</td>
  <td>0.3233</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>90.5</td>
  <td>1439.9</td>
  <td>84.2</td>
  <td>2.34</td>
  <td>0.0295</td>
  <td>0.1255</td>
  <td>0.2347</td>
  <td>0.3221</td>
  <td>0.3912</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>11.1</td>
  <td>32.4</td>
  <td>10.8</td>
  <td>1.30</td>
  <td>0.0000</td>
  <td>0.0002</td>
  <td>0.0005</td>
  <td>0.0007</td>
  <td>0.0009</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>75.8</td>
  <td>338.4</td>
  <td>53.0</td>
  <td>2.39</td>
  <td>0.0205</td>
  <td>0.0828</td>
  <td>0.1425</td>
  <td>0.1791</td>
  <td>0.2028</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-256D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>38.7</td>
  <td>175.0</td>
  <td>32.2</td>
  <td>1.97</td>
  <td>0.0421</td>
  <td>0.0638</td>
  <td>0.0742</td>
  <td>0.0815</td>
  <td>0.0863</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-512D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>44.5</td>
  <td>209.0</td>
  <td>43.5</td>
  <td>2.06</td>
  <td>0.0651</td>
  <td>0.1088</td>
  <td>0.1297</td>
  <td>0.1422</td>
  <td>0.1516</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-SIFT</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>75.3</td>
  <td>285.9</td>
  <td>44.8</td>
  <td>2.81</td>
  <td>0.1219</td>
  <td>0.1727</td>
  <td>0.1925</td>
  <td>0.2039</td>
  <td>0.2102</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are HOG (as in SIFT).</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ORB (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>92.1</td>
  <td>3463.2</td>
  <td>76.8</td>
  <td>2.80</td>
  <td>0.2696</td>
  <td>0.3522</td>
  <td>0.3913</td>
  <td>0.4239</td>
  <td>0.4531</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (NN matcher)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-10</span></td>
  <td>F</td>
  
  <td>89.8</td>
  <td>2453.7</td>
  <td>79.8</td>
  <td>2.72</td>
  <td>0.2058</td>
  <td>0.2932</td>
  <td>0.3444</td>
  <td>0.3862</td>
  <td>0.4224</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (custom matcher)</span><br /><span class="boards_subtitle">kp:8000, match:sift-aid<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-29</span></td>
  <td>F/M</td>
  
  <td>89.4</td>
  <td>2358.0</td>
  <td>78.0</td>
  <td>2.67</td>
  <td>0.1967</td>
  <td>0.2852</td>
  <td>0.3357</td>
  <td>0.3700</td>
  <td>0.4064</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-ContextDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>99.3</td>
  <td>6204.4</td>
  <td>99.0</td>
  <td>3.26</td>
  <td>0.4145</td>
  <td>0.5301</td>
  <td>0.5961</td>
  <td>0.6511</td>
  <td>0.6984</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">ContextDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features are quantized to uint8 and extracted from the code provided by the authors.</td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-GeoDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-08</span></td>
  <td>F</td>
  
  <td>99.2</td>
  <td>5465.5</td>
  <td>96.2</td>
  <td>3.31</td>
  <td>0.4321</td>
  <td>0.5493</td>
  <td>0.6165</td>
  <td>0.6710</td>
  <td>0.7125</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features extracted from the code provided by the authors.</td>
  <td class="hidden_value"><a href="https://arxiv.org/abs/1807.06294">https://arxiv.org/abs/1807.06294</a></td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; GeoDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>99.2</td>
  <td>5972.0</td>
  <td>97.2</td>
  <td>3.50</td>
  <td>0.4401</td>
  <td>0.5554</td>
  <td>0.6234</td>
  <td>0.6820</td>
  <td>0.7212</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; HardNet</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>99.7</td>
  <td>6868.1</td>
  <td>98.2</td>
  <td>3.52</td>
  <td>0.4578</td>
  <td>0.5680</td>
  <td>0.6376</td>
  <td>0.6851</td>
  <td>0.7191</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; L2-Net</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>99.3</td>
  <td>6480.5</td>
  <td>98.2</td>
  <td>3.33</td>
  <td>0.3764</td>
  <td>0.4962</td>
  <td>0.5832</td>
  <td>0.6330</td>
  <td>0.6822</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>96.4</td>
  <td>4750.2</td>
  <td>89.8</td>
  <td>3.11</td>
  <td>0.3349</td>
  <td>0.4361</td>
  <td>0.4979</td>
  <td>0.5367</td>
  <td>0.5821</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; TFeat</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>98.8</td>
  <td>5944.1</td>
  <td>96.5</td>
  <td>3.23</td>
  <td>0.3548</td>
  <td>0.4763</td>
  <td>0.5503</td>
  <td>0.6128</td>
  <td>0.6506</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint (default)</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>93.5</td>
  <td>1027.1</td>
  <td>88.5</td>
  <td>3.59</td>
  <td>0.3638</td>
  <td>0.4593</td>
  <td>0.5128</td>
  <td>0.5560</td>
  <td>0.5902</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We extract features with the default parameters and use however many are returned (about 1200 on average). Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>92.9</td>
  <td>830.4</td>
  <td>85.2</td>
  <td>3.58</td>
  <td>0.3786</td>
  <td>0.4686</td>
  <td>0.5174</td>
  <td>0.5500</td>
  <td>0.5801</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>93.6</td>
  <td>1468.0</td>
  <td>90.5</td>
  <td>3.51</td>
  <td>0.3513</td>
  <td>0.4497</td>
  <td>0.5199</td>
  <td>0.5611</td>
  <td>0.5891</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>49.7</td>
  <td>184.0</td>
  <td>35.2</td>
  <td>2.32</td>
  <td>0.1264</td>
  <td>0.1528</td>
  <td>0.1638</td>
  <td>0.1699</td>
  <td>0.1742</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>89.4</td>
  <td>434.7</td>
  <td>69.5</td>
  <td>3.34</td>
  <td>0.3006</td>
  <td>0.3722</td>
  <td>0.4048</td>
  <td>0.4272</td>
  <td>0.4503</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>94.8</td>
  <td>4882.2</td>
  <td>94.5</td>
  <td>3.30</td>
  <td>0.2568</td>
  <td>0.3925</td>
  <td>0.4707</td>
  <td>0.5367</td>
  <td>0.5743</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SURF (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>92.4</td>
  <td>2982.1</td>
  <td>80.2</td>
  <td>2.85</td>
  <td>0.2626</td>
  <td>0.3615</td>
  <td>0.4159</td>
  <td>0.4536</td>
  <td>0.4859</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  

</tbody>
</table>



<hr>
<a name="usc"></a>










<table class="display leaderboard_mvs" width="99%" cellspacing="0">
<thead>
<tr>
  <th colspan="13" style="text-align: center;" class="sorter-false">MVS &mdash; sequence &#39;united_states_capitol&#39;</th>
</tr>
<tr>
  
  <th class="all align_left" width="99%">Method</th>
  <th class="all">Date</th>
  <th class="all"><span class="balloon" data-balloon="F: Features only.&#10;M: Features and custom matches." data-balloon-break data-balloon-pos="up">Type</span></th>
  
  <th class="all"><span class="balloon" data-balloon="Ratio of images successfully&#10;registered (min: 3)." data-balloon-break data-balloon-pos="up">Ims (%)</span></th>
  <th class="all"><span class="balloon" data-balloon="Average number of 3D points&#10;in the reconstruction." data-balloon-break data-balloon-pos="up">#Pts</span></th>
  <th class="all"><span class="balloon" data-balloon="Success ratio in the&#10;3D reconstruction (%)." data-balloon-break data-balloon-pos="up">SR</span></th>
  <th class="all"><span class="balloon" data-balloon="Average Track Length&#10;(# of observations per 3D point)." data-balloon-break data-balloon-pos="up">TL</span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 5-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>5<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 10-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>10<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 15-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>15<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 20-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>20<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Mean Average Precision&#10;at a 25-degree threshold." data-balloon-break data-balloon-pos="up">mAP<sup>25<sup>o</sup></sup></span></th>
  <th class="all"><span class="balloon" data-balloon="Average Trajectory Error." data-balloon-break data-balloon-pos="up">ATE</span></th>
  
  <th class="none hidden_key">By</th>
  <th class="none hidden_key">Details</th>
  <th class="none hidden_key">Link</th>
  <th class="none hidden_key">Contact</th>
  <th class="none hidden_key">Updated</th>
  <th class="none hidden_key">Descriptor size</th>
</tr>
</thead>
<tbody>































<tr>
  
  <td class="align_left"><span class="boards_title">AKAZE (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>91.0</td>
  <td>1813.4</td>
  <td>86.5</td>
  <td>2.91</td>
  <td>0.0720</td>
  <td>0.1280</td>
  <td>0.1847</td>
  <td>0.2412</td>
  <td>0.2880</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">AKAZE, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SphereDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>89.0</td>
  <td>928.2</td>
  <td>82.5</td>
  <td>2.76</td>
  <td>0.0510</td>
  <td>0.1023</td>
  <td>0.1606</td>
  <td>0.2114</td>
  <td>0.2634</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of AKAZE detector, and for each keypoint we extract a descriptor via CNN.</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">Brisk &#43; SSS</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-14</span></td>
  <td>F</td>
  
  <td>80.4</td>
  <td>701.1</td>
  <td>71.0</td>
  <td>2.48</td>
  <td>0.0194</td>
  <td>0.0518</td>
  <td>0.0936</td>
  <td>0.1299</td>
  <td>0.1688</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">We use OpenCV&#39;s implementation of brisk detector with the default settings, and for each image there are at most 8K keypoints. For each keypoint, we extract a descriptor via a CNN model.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (single scale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>88.8</td>
  <td>1326.9</td>
  <td>86.8</td>
  <td>2.69</td>
  <td>0.0220</td>
  <td>0.0745</td>
  <td>0.1321</td>
  <td>0.1910</td>
  <td>0.2469</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Single-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">D2-Net (multiscale)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>89.8</td>
  <td>1647.0</td>
  <td>89.5</td>
  <td>2.76</td>
  <td>0.0207</td>
  <td>0.0729</td>
  <td>0.1323</td>
  <td>0.1987</td>
  <td>0.2639</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. Multi-scale features, brute-force nearest neighbour matching. Paper: https://dsmn.ml/files/d2-net/d2-net.pdf</td>
  <td class="hidden_value"><a href="https://github.com/mihaidusmanu/d2-net">https://github.com/mihaidusmanu/d2-net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">512 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>81.0</td>
  <td>423.7</td>
  <td>81.0</td>
  <td>2.35</td>
  <td>0.0025</td>
  <td>0.0256</td>
  <td>0.0602</td>
  <td>0.0995</td>
  <td>0.1363</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>83.0</td>
  <td>653.1</td>
  <td>83.8</td>
  <td>2.30</td>
  <td>0.0044</td>
  <td>0.0264</td>
  <td>0.0615</td>
  <td>0.1043</td>
  <td>0.1467</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>61.2</td>
  <td>88.2</td>
  <td>37.3</td>
  <td>2.31</td>
  <td>0.0007</td>
  <td>0.0043</td>
  <td>0.0112</td>
  <td>0.0208</td>
  <td>0.0286</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">DELF</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-05</span></td>
  <td>F</td>
  
  <td>72.0</td>
  <td>207.2</td>
  <td>68.2</td>
  <td>2.35</td>
  <td>0.0014</td>
  <td>0.0171</td>
  <td>0.0375</td>
  <td>0.0634</td>
  <td>0.0849</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">DELF features for object retrieval, trained on the Google Landmarks dataset. Paper: https://arxiv.org/abs/1612.06321</td>
  <td class="hidden_value"><a href="https://github.com/tensorflow/models/tree/master/research/delf">https://github.com/tensorflow/models/tree/master/research/delf</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">40 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-256D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-07</span></td>
  <td>F</td>
  
  <td>3.7</td>
  <td>19.0</td>
  <td>5.2</td>
  <td>0.60</td>
  <td>0.0001</td>
  <td>0.0002</td>
  <td>0.0002</td>
  <td>0.0002</td>
  <td>0.0003</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-512D</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>25.9</td>
  <td>62.5</td>
  <td>12.5</td>
  <td>1.74</td>
  <td>0.0000</td>
  <td>0.0004</td>
  <td>0.0009</td>
  <td>0.0013</td>
  <td>0.0017</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are computed with the interpolation of the VGG pool3 feature map on the detected keypoints.</td>
  <td class="hidden_value"><a href="TBA">TBA</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ELF-SIFT</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>28.2</td>
  <td>79.4</td>
  <td>34.2</td>
  <td>1.77</td>
  <td>0.0017</td>
  <td>0.0038</td>
  <td>0.0055</td>
  <td>0.0067</td>
  <td>0.0080</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Anonymous</td>
  <td class="hidden_value">ELF detector: Keypoints are local maxima of a saliency map generated by the gradient of a feature map with respect to the image of a pre-trained CNN. Descriptors are HOG (as in SIFT).</td>
  <td class="hidden_value"><a href="N/A">N/A</a></td>
  <td class="hidden_value"><a href="mailto:Anonymous">Anonymous</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">ORB (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>74.6</td>
  <td>1198.0</td>
  <td>60.0</td>
  <td>2.39</td>
  <td>0.0159</td>
  <td>0.0394</td>
  <td>0.0613</td>
  <td>0.0846</td>
  <td>0.1068</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">ORB, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (NN matcher)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-10</span></td>
  <td>F</td>
  
  <td>78.1</td>
  <td>550.6</td>
  <td>70.2</td>
  <td>2.51</td>
  <td>0.0088</td>
  <td>0.0263</td>
  <td>0.0502</td>
  <td>0.0817</td>
  <td>0.1114</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-AID (custom matcher)</span><br /><span class="boards_subtitle">kp:8000, match:sift-aid<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-29</span></td>
  <td>F/M</td>
  
  <td>77.4</td>
  <td>521.8</td>
  <td>68.5</td>
  <td>2.49</td>
  <td>0.0060</td>
  <td>0.0208</td>
  <td>0.0491</td>
  <td>0.0757</td>
  <td>0.1008</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Mariano Rodríguez, Gabriele Facciolo, Rafael Grompone Von Gioi, Pablo Musé, Jean-Michel Morel, Julie Delon</td>
  <td class="hidden_value">We extract the keypoints using OpenCV&#39;s implementation of SIFT. The AID descriptors are computed with a CNN from patches extracted at each keypoint location, the result is a binary descriptor of 6272 bits. The matching is computed as the Hamming distance between the descriptors, with the decision threshold set at 4000. Preprint: https://hal.archives-ouvertes.fr/hal-02016010. Code: https://github.com/rdguez-mariano/sift-aid</td>
  <td class="hidden_value"><a href="https://hal.archives-ouvertes.fr/hal-02016010">https://hal.archives-ouvertes.fr/hal-02016010</a></td>
  <td class="hidden_value"><a href="mailto:facciolo@cmla.ens-cachan.fr">facciolo@cmla.ens-cachan.fr</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">6272 bits</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-ContextDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-09</span></td>
  <td>F</td>
  
  <td>91.9</td>
  <td>1870.7</td>
  <td>90.8</td>
  <td>2.77</td>
  <td>0.0663</td>
  <td>0.1178</td>
  <td>0.1735</td>
  <td>0.2268</td>
  <td>0.2712</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">ContextDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features are quantized to uint8 and extracted from the code provided by the authors.</td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 uint8</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT-GeoDesc-GitHub</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-05-08</span></td>
  <td>F</td>
  
  <td>90.8</td>
  <td>1685.8</td>
  <td>89.0</td>
  <td>2.80</td>
  <td>0.0634</td>
  <td>0.1011</td>
  <td>0.1497</td>
  <td>0.2003</td>
  <td>0.2523</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Zixin Luo</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search. Features extracted from the code provided by the authors.</td>
  <td class="hidden_value"><a href="https://arxiv.org/abs/1807.06294">https://arxiv.org/abs/1807.06294</a></td>
  <td class="hidden_value"><a href="mailto:zluoag@cse.ust.hk">zluoag@cse.ust.hk</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; GeoDesc</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>91.3</td>
  <td>2088.3</td>
  <td>90.2</td>
  <td>2.86</td>
  <td>0.0640</td>
  <td>0.1096</td>
  <td>0.1616</td>
  <td>0.2239</td>
  <td>0.2710</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">GeoDesc extracted on SIFT keypoints. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/lzx551402/geodesc">https://github.com/lzx551402/geodesc</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; HardNet</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>92.5</td>
  <td>2496.1</td>
  <td>93.5</td>
  <td>2.87</td>
  <td>0.0659</td>
  <td>0.1175</td>
  <td>0.1794</td>
  <td>0.2339</td>
  <td>0.2888</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">HardNet extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/DagnyT/hardnet">https://github.com/DagnyT/hardnet</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; L2-Net</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>91.1</td>
  <td>2193.0</td>
  <td>93.0</td>
  <td>2.87</td>
  <td>0.0671</td>
  <td>0.1217</td>
  <td>0.1778</td>
  <td>0.2377</td>
  <td>0.2895</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">L2-Net extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/yuruntian/L2-Net">https://github.com/yuruntian/L2-Net</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>82.8</td>
  <td>1322.7</td>
  <td>79.8</td>
  <td>2.65</td>
  <td>0.0441</td>
  <td>0.0756</td>
  <td>0.1131</td>
  <td>0.1496</td>
  <td>0.1827</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SIFT, as implemented in OpenCV. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SIFT &#43; TFeat</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>90.0</td>
  <td>1949.4</td>
  <td>87.8</td>
  <td>2.74</td>
  <td>0.0448</td>
  <td>0.0903</td>
  <td>0.1402</td>
  <td>0.1946</td>
  <td>0.2422</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">T-Feat extracted on SIFT keypoints. Number of keypoints: 8000 per image. Models trained on the Liberty sequence of the Brown dataset. We use slightly larger paches than specified for SIFT (scale multiplying factor 16/12). Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/vbalnt/tfeat">https://github.com/vbalnt/tfeat</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">128 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint (default)</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>82.5</td>
  <td>376.4</td>
  <td>75.0</td>
  <td>2.92</td>
  <td>0.0608</td>
  <td>0.1068</td>
  <td>0.1470</td>
  <td>0.1855</td>
  <td>0.2230</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We extract features with the default parameters and use however many are returned (about 1200 on average). Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:1024, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>79.4</td>
  <td>335.0</td>
  <td>74.2</td>
  <td>2.87</td>
  <td>0.0524</td>
  <td>0.0956</td>
  <td>0.1302</td>
  <td>0.1667</td>
  <td>0.1983</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:2048, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>84.4</td>
  <td>537.5</td>
  <td>77.5</td>
  <td>2.87</td>
  <td>0.0563</td>
  <td>0.1054</td>
  <td>0.1501</td>
  <td>0.1934</td>
  <td>0.2285</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:256, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>26.1</td>
  <td>75.3</td>
  <td>18.0</td>
  <td>1.75</td>
  <td>0.0007</td>
  <td>0.0011</td>
  <td>0.0016</td>
  <td>0.0017</td>
  <td>0.0023</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:512, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>66.6</td>
  <td>152.6</td>
  <td>57.0</td>
  <td>2.59</td>
  <td>0.0231</td>
  <td>0.0409</td>
  <td>0.0585</td>
  <td>0.0746</td>
  <td>0.0871</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SuperPoint</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-26</span></td>
  <td>F</td>
  
  <td>86.7</td>
  <td>1606.2</td>
  <td>84.8</td>
  <td>2.63</td>
  <td>0.0276</td>
  <td>0.0716</td>
  <td>0.1208</td>
  <td>0.1717</td>
  <td>0.2196</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SuperPoint features. If necessary, we downsample the images so that the largest dimension is at most 1024 pixels. We lower the default detection threshold to take the number of features indicated in the label. Feature matching done with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork">https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">256 float32</td>
</tr>
  































<tr>
  
  <td class="align_left"><span class="boards_title">SURF (OpenCV)</span><br /><span class="boards_subtitle">kp:8000, match:nn<span></td>
  <td data-sorter="shortDate" data-date-format="yyyymmdd"><span class="nolinebreak">19-04-24</span></td>
  <td>F</td>
  
  <td>82.8</td>
  <td>967.7</td>
  <td>73.0</td>
  <td>2.52</td>
  <td>0.0190</td>
  <td>0.0419</td>
  <td>0.0760</td>
  <td>0.1135</td>
  <td>0.1451</td>
  
  <td>&mdash;</td>
  
  <td class="hidden_value">Challenge organizers</td>
  <td class="hidden_value">SURF, as implemented in OpenCV. Number of keypoints: 8000 per image. Feature matching with brute-force nearest-neighbour search.</td>
  <td class="hidden_value"><a href="https://opencv.org">https://opencv.org</a></td>
  <td class="hidden_value"><a href="mailto:imagematching@uvic.ca">imagematching@uvic.ca</a></td>
  <td class="hidden_value">N/A</td>
  <td class="hidden_value">TBA</td>
</tr>
  

</tbody>
</table>


</p>

  </div>
</section>
<section id="tag-pane" class="row meta">
  
</section>








<section id="menu-pane" class="row menu text-center">
  
  
  <span><a class="menu-item" href="https://image-matching-workshop.github.io/breakdown/phototourism/mvs/by_bag/">&lt; prev | </a></span>
  
  
  <span><a class="menu-item" href="/breakdown">breakdown</a></span>
  
  
  
  <h4 class="text-center"><a class="menu-item" href="https://image-matching-workshop.github.io/">home</a></h4>
</section>



<footer class="row text-center footer">
  
  
</footer>

</div>




<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-62863910-6', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="js/main.js"></script>

</script>
</body>
</html>


